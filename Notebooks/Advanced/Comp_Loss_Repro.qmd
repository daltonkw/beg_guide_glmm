---
title: "Workers' Compensation: Losses"
format:
  html:
    toc: true
    number-sections: true
---

Reproduce & expand Antonio & Beirlant, _Actuarial Statistics With Generalized Linear Mixed Models_, section 4.2.3.

## Setup & Reproducibility

```{r}
#| label: setup
#| echo: false
#| warning: false

library(tidyverse)
library(glmmTMB)
library(CASdatasets)
library(DHARMa)
library(performance)
library(broom)
library(broom.mixed)
library(emmeans)
library(brms)
library(patchwork)
library(GGally)
library(MASS)
library(tidybayes)
library(bayesplot)
library(gt)

library(future)
plan(multicore, workers = 4)
options(future.globals.maxSize = 4 * 1024^3)

data(usworkcomp,  package = "CASdatasets")

source("R/pred_boxplots_group.r")
```

We follow the models the authors considered in their paper for Workers' Compensation loss data and expand on their analysis.

$$
\begin{aligned} 
Y_{i j} \mid \mathbf{b}_i & \sim \Gamma\left(\nu, \mu_{i j} / \nu\right) \\
\text{where} \log \left(\mu_{i j}\right) & =\log \left(\operatorname{PAYROLL}_{i j}\right)+\beta_0+b_{i, 0} \\
\text{versus} \log \left(\mu_{i j}\right) & =\log \left(\operatorname{PAYROLL}_{i j}\right)+\beta_0+\beta_1 \operatorname{Year}_{i j}+b_{i, 0} \\
\text{and} \log \left(\mu_{i j}\right) & =\log \left(\operatorname{PAYROLL}_{i j}\right)+\beta_0+\beta_1 \operatorname{Year}_{i j}+b_{i, 0}+b_{i, 1} \operatorname{Year}_{i j}
\end{aligned}
$$

Here we're fitting a Positive only Loss with offset of Log(Payroll).  That is, the expected total loss for class _i_, year _j_, is proportional to the exposure base (here Payroll) and the multplicative factor $\exp(\eta_{ij})$ represents the loss cost rate per unit of Payroll.  So, a one-part Pure Premium (aggregate loss cost) type model.

$$
E\left[\text { Loss }_{i j}\right]=\text { Payroll }_{i j} \times \exp \left(\eta_{i j}\right)
$$

## Data

Data from Klugman (1992) from CASdatasets.

```{r}
#| label: workcomp-data
#| echo: false
comp <- usworkcomp |> 
    dplyr::rename(
        Class = CL,
        Year = YR,
        Payroll = PR,
        Loss = LOSS
    )

comp |> head() |> 
  gt() |> 
  gt::fmt_number(columns = c(Payroll, Loss), decimals = 0)
```

## EDA

Typical Workers' Compensation panel loss data.

```{r}
#| label: loss-histogram
#| fig-cap: Histogram of Loss (whole database)
comp |> ggplot() +
    geom_histogram(aes(x = Loss), bins = 10, colour = "white", fill = "lightblue") +
    labs(x = "Loss", y = "", title = "Workers' Compensation Data (Losses)") +
    theme_minimal() +
    theme(axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    scale_y_continuous(breaks = seq(0, max(comp$Loss, na.rm = TRUE), by = 100))
```

With a clear year to year correlation.

```{r}
#| label: yearly-pairsplot
#| fig-cap: Scatterplot matrix of losses, Year i against Year j
wide_data <- comp %>%
  pivot_wider(
    id_cols = Class,
    names_from = Year,
    values_from = Loss,
    names_prefix = "Year"
  ) %>%
    dplyr::select(-Class)

GGally::ggpairs(
  data = wide_data,
  columns = 1:7,
  upper = "blank",
  lower = list(continuous = "points"),
  diag = NULL
) +
  theme_bw() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )
```

```{r}
#| label: gamma-fitdist
#| echo: false
pos_loss <- comp$Loss[comp$Loss > 0]
mean_loss <- mean(pos_loss)
var_loss <- var(pos_loss)
shape_start <- mean_loss^2 / var_loss
rate_start <- mean_loss / var_loss
fit <- fitdistrplus::fitdist(
    pos_loss, "gamma", method = "mme", start = list(shape = shape_start, rate = rate_start))
```

Gamma seems like a good fit.

```{r}
#| label: gamma-qqplot
#| fig-cap: QQPlot for Gamma
fitdistrplus::qqcomp(fit, main = "Workers' Compensation Insurance Losses")
```

- Drop zero-loss observations from the data set.
- Standardize Payroll to units of $10,000, `Payroll_M`.
- Center the Year, `Year_C`.

```{r}
#| label: standardize-center-data
#| echo: false
comp_loss <- comp |> 
    dplyr::filter(Loss > 0) |> 
    dplyr::mutate(Payroll_M = Payroll/10000,
                  Year_C = as.integer(Year - 4))

comp_loss <- within(comp_loss, {
    Class <- factor(Class)
    log_payroll <- log(Payroll_M)
})

comp_loss |> head() |> 
  gt() |> 
  gt::fmt_number(columns = c(Payroll, Loss), decimals = 0) |> 
  gt::fmt_number(columns = c(Payroll_M, log_payroll), decimals = 2)
```

A note about centering the Year here:

**Without centering:**
$$
\log \left(\mu_{i j}\right)=\log (\text{Payroll})+\beta_0+\beta_1 \text{Year}_j+b_{0 i}+b_{1 i} \text{Year}_j
$$

- $\beta_0$ is our expected log-loss rate at Year 0 which doesn't have any real meaning.  Year 0 is arbitrary.
- The slope interpretation is fine and won't change if we center the data.
- Makes intercept and slope highly correlated.

**With centering:**
$$
\log \left(\mu_{i j}\right)=\log (\text{Payroll})+\beta_0+\beta_1 \text{Year_c}_j+b_{0 i}+b_{1 i} \text{Year_c}_j
$$

- Centering doesn't change any of the meaning.  It simply anchors everything at a reference point of the average year (here Year 4).

## Model Specification

"Correlation between observations on the same subject arises because they share the same random effects $\mathbf{b}_i$."

**Loss** is the response variable and **Payroll** is used as an _offset_.  **Class** is the group.  We're fitting models using both the frequentists and bayesian frameworks with `glmmTMB` and `brms` for comparison & illustrative purposes.

- Gamma GLMM (log link)
- response = Loss
- offset = log(Payroll)

$$
\begin{align*}
Y_{i j} \mid \mathbf{b}_{i} & \sim \Gamma\left(\nu, \mu_{i j} / \nu\right) \\
\log \left(\mu_{i j}\right) & =\log \left(\mathrm{PAYROLL}_{i j}\right)+\beta_{0}+b_{i, 0} \\
\log \left(\mu_{i j}\right) & =\log \left(\mathrm{PAYROLL}_{i j}\right)+\beta_{0}+\beta_{1} \mathrm{Year}_{i j}+b_{i, 0} \\
\log \left(\mu_{i j}\right) & =\log \left(\mathrm{PAYROLL}_{i j}\right)+\beta_{0}+\beta_{1} \mathrm{Year}_{i j}+b_{i, 0}+b_{i, 1} \mathrm{Year}_{i j}
\end{align*}
$$

### Model 1

Fixed and random intercepts only.

$\log \left(\mu_{i j}\right) =\log \left(\mathrm{PAYROLL}_{i j}\right)+\beta_{0}+b_{i, 0}$

```{r}
#| label: model-25-glmmTMB
mod_0_tmb <- glmmTMB(
    formula = Loss ~ 1 + (1 | Class),
    data = comp_loss,
    family = Gamma(link = "log"),
    offset = comp_loss$log_payroll,
    REML = TRUE
)
```

**predictive simulations from the fitted model (a parametric bootstrap)**

Useful to get a sense of the structure of the model.  Somewhat analogous to prior predictive checks in Bayesian modeling.

In `glmmTMB` the Gamma variance is:

$$
\text{Var}(Y_{i} \mid \mu_i) = \phi \mu_i^2
$$

Equivalently, a Gamma (shape = $k$, scale = $\theta$) with mean $k\theta=\mu$ and variance $k \theta^2 = \phi \mu^2$ implies:

$$
k = \frac{1}{\phi}, \quad \theta = \phi \mu
$$

To simulate $y^{\text{rep}}$ for each row:
- get $\mu_i$ from `predict(mod_0_tmb, type = "response")`
- get $\phi$ then set shape = 1/phi and scale = phi * mu_i `sigma(mod_0_tmb)^2`
- draw `rgamma(n, shape, scale)`

```{r}
#| label: predictive-simulations-model-25-glmmTMB

# fitted means; with random effects (conditional)
mu_cond <- predict(mod_0_tmb, type = "response")
# fitted means; marginal over random effects (no random effects)
mu_marg <- predict(mod_0_tmb, type = "response", re.form = NA) 
phi <- sigma(mod_0_tmb)^2

shape <- 1 / phi
scale_cond <- phi * mu_cond
scale_marg <- phi * mu_marg

# mu_cond when you want simulation including realized class effects
# mu_marg when you want simulation marginalizing over class effects

n <- nrow(comp_loss)
set.seed(1055)
mod_0_tmb_sim_df <- dplyr::tibble(
  Class = comp_loss$Class,
  Year = comp_loss$Year,
  Payroll_M = comp_loss$Payroll_M,
  mu_cond = mu_cond,
  mu_marg = mu_marg,
  rp1_cond = rgamma(n, shape = shape, scale = scale_cond),
  rp2_cond = rgamma(n, shape = shape, scale = scale_cond),
  rp3_cond = rgamma(n, shape = shape, scale = scale_cond),
  rp1_marg = rgamma(n, shape = shape, scale = scale_marg),
  rp2_marg = rgamma(n, shape = shape, scale = scale_marg),
  rp3_marg = rgamma(n, shape = shape, scale = scale_marg)
)
```

The predictive simulations, marginalizing out class effects are consistent with the fitted means.

```{r}
#| label: predictive-simulations-plot-marginal
#| fig-cap: Predictive simulations from Model 1 (marginalizing out class effects)

marg_loss <- mod_0_tmb_sim_df |> 
  dplyr::select(Year, mu_marg, rp1_marg, rp2_marg, rp3_marg) |>
  tidyr::pivot_longer(
    cols = c(mu_marg, rp1_marg, rp2_marg, rp3_marg),
    names_to = "series",
    values_to = "loss"
  ) |> 
  dplyr::mutate(
    Year = factor(Year),
    series = factor(
      series,
      levels = c("mu_marg", "rp1_marg", "rp2_marg", "rp3_marg"),
      labels = c("Fit (μ̂, marginal)","Sim 1","Sim 2","Sim 3")
    )
  )

ggplot2::ggplot(marg_loss, aes(x = Year, y = loss, fill = series)) +
  geom_boxplot(position = position_dodge(width = 0.85), alpha = 0.2, outlier.alpha = 0.15) +
  scale_y_log10() +
  theme_minimal()
```

When we look at the conditional predictive simulations that include class effects for several classes we see the fitted distributions don't appear to reflect any yearly increase.

Next we consider the conditional predictive simulations including class effects for Class 10 and Class 43.

```{r}
#| label: predictive-simulations-plot-conditional
#| fig-cap: Predictive simulations from Model 1 (including class effects)

n <- nrow(mod_0_tmb_sim_df)
sigma <- sigma(mod_0_tmb)^2
shape <- 1 / phi
scale_cond <- phi * mod_0_tmb_sim_df$mu_cond

k <- 100
set.seed(89)

# matrix of draws
draw_mat <- matrix(
  rgamma(n * k, shape = shape, scale = rep(scale_cond, each = k)),
  nrow = n,
  ncol = k,
  byrow = FALSE
)

# long format with indices
sims_long <- as_tibble(draw_mat, .name_repair = ~ paste0("draw_", seq_along(.))) |>
  dplyr::mutate(.row = row_number()) |>
  tidyr::pivot_longer(starts_with("draw_"), names_to = "draw", values_to = "loss") |>
  dplyr::left_join(
    comp_loss |> dplyr::mutate(.row = row_number()) |> dplyr::select(.row, Class, Year, Payroll_M),
    by = ".row"
  )

fit_df <- mod_0_tmb_sim_df |> 
  dplyr::filter(Class %in% c(10, 43)) |> 
  dplyr::group_by(Class, Year) |> 
  dplyr::summarize(mu_cond = first(mu_cond), .groups = "drop")

plot_df <- sims_long |> 
  dplyr::filter(Class %in% c(10, 43)) |> 
  dplyr::mutate(Year = factor(Year), Class = factor(Class))

ggplot2::ggplot(plot_df, aes(x = Year, y = loss)) +
  geom_boxplot(fill = "grey85", outlier.alpha = 0.2) +
  geom_point(data = fit_df, aes(y = mu_cond), color = "red", size = 2) +
  geom_line(data = fit_df, aes(y = mu_cond, group = Class), color = "red", linewidth = 0.6) +
  scale_y_log10() +
  facet_wrap(~ Class, scales = "free_y") +
  labs(x = "Year", y = "Loss (conditional)",
       title = "Predictive simulations (conditional): boxplots by Year",
       subtitle = paste0("K = ", k, " draws per observation; red = fitted mean")) +
  theme_minimal()
```

Each boxplot summarizes the _conditional predictive distribution_ for a given class-year cell, given:

$$
Y_{ij}^{\text{rep}} \sim \text{Gamma}\left(\text{shape}=1/\phi, \text{scale}=\phi \mu_{ij} \right)
$$

where $\mu_{ij}$ includes the realized random effect for that class.

- The red line is the model's fitted conditional mean, $\mu_{ij}$ by Year for that Class.
- For theses Classes, simulated 100 times each per year, so each plot shows what kind of spread the model expects _within that class-year_, conditional on the fitted mean and variance.
- If the model fit well (the fitted mean) should cut roughly through the middle of each boxplot (the predictive median).

Since this a **random-intercept-only** Gamma GLMM:

$$
\log \left(\mu_{i j}\right)=\beta_0+b_{0 i}, \quad b_{0 i} \sim N\left(0, \sigma_b^2\right)
$$

- We don't have any time trend, either globally of within class.
- The predictive simulations therefore draw from a stationary distribution by class, which is not what we see from the empirical fit - the empirical pattern looks time varying at least for some classes.

**bayesian models with brms**

We'll walk through the use of `brms` to fit the same model in a Bayesian framework.  Developing and checking our priors as well as fitting the model and checking the posterior predictive fit.  This isn't a fully principled Bayesian analysis but rather an illustrative example of how to use `brms` for this type of model.

`brms` gets _sensible_ priors for us by default but it's always good to check what they are and whether they make sense for our application.

```{r}
#| label: model-25-brms
brms::get_prior(
    formula = bf(Loss ~ offset(log_payroll) + 1 + (1 | Class)),
    data = comp_loss,
    family = Gamma(link = "log")
)
```

- Intercept: weakly informative prior for the _global intercept_ (on the log scale).  Centered near the sample mean of `log(Loss)` minus the mean offset.  This is $\beta_0$.
- sd_Intercept_Class: prior for the **standard deviation of the random intercepts** for each class.

This is the specific parameter $\sigma_b$ in $b_i \sim N\left(\sigma_b^2\right)$.

- shape: prior for the Gamma **shape parameter** k

**bayesian model fitting prior predictive checks**

_What kinds of data would my model believe are plausible, before seeing any data?_

- Fit brms model with sample_prior = "only"
- Draw prior predictive replicates
- Visual checks
- Numeric checks

```{r}
#| label: fit-model-25-brms
priors <- c(
    prior(student_t(3, 4.883, 2.5), class = Intercept),
    prior(student_t(3, 0, 2.5), class = sd, group = "Class", coef = "Intercept"),
    prior(lognormal(log(3.3), 0.4), class = shape)
)

mod_0_brms <- brms::brm(
    formula = bf(Loss ~ offset(log_payroll) + 1 + (1 | Class)),
    data = comp_loss,
    family = Gamma(link = "log"),
    prior = priors,
    sample_prior = "only",
    chains = 4,
    cores = 4,
    warmup = 1000,
    iter = 6000,
    threads = threading(2),
    seed = 67,
    backend = "rstan",
    refresh = 0,
    file = "Models/workcomp/_mod-0-brms"
)
```

Our model specifies three major components:

$$
\begin{aligned}
y_i & \sim \operatorname{Gamma}\left(\mu_i, \phi\right) \\
\log \left(\mu_i\right) & =\log \left(\operatorname{payroll}_i\right)+\beta_0+u_{j[i]} \\
u_j & \sim N\left(0, \sigma_{\text {Class }}^2\right)
\end{aligned}
$$

with priors specified for all unknown paramaters, $\beta_0, \phi, \sigma_{\text {Class }}$.

These priors jointly define the _prior predictive distribution_ of $\tilde{y}$.  Here the marginalization (or equivalently, expectation) is taken with respect to the prior distribution instead of the posterior distribution.

$$
p(\tilde{y})=\int p(\tilde{y} \mid \theta) p(\theta) d \theta
$$

`sample_prior = "only"` tells `brms` to:
- draw parameters $\theta = (\beta,\sigma_{\text{Class}}, \phi)$ from their priors only
- simulate fake data $\tilde{y}$ from $p(\tilde{y} \mid \theta)$ using the model's structure
- produce draws of $\tilde{y}_i$ that reflect what the model believes are plausible data before seeing any actual data

Then, by comparing the distribution of simulated $\tilde{y}$ to the observed data $y$, we can assess the _prior realism_.

What we're checking:

- **Scale realism:**  Are simulated losses roughly in the same order of magnitude as observed losses?
- **Variation realism:**  Are the simulated within-Class and between-Class spreads reasonable?
- **Structural realism:**  Do the simulated means include patterns we expect to see in the data (e.g., time trends, heterogeneity across Classes)?

_If we knew nothing about the data except our model assumptions and priors, what data would we expect to see?_

Our model implies
$$
\log \mu_i=\log \left(\text{Payroll}_{10, i}\right)+\beta_0+u_{j[i]}, \quad \operatorname{rate}_{10 k, i} \equiv \frac{\mu_i}{\operatorname{Payroll}_{10, i}}=\exp \left(\beta_0+u_{j[i]}\right)
$$

The priors above are extremely diffuse, leading to large values for rate before we've seen any data.  This violates our sense of **scale realism**.

```{r}
#| label: mod-25-brms-ppc-prior
#| fig-cap: Prior Predictive Check: Losses
pp_check(mod_0_brms, ndraws = 1, type = "hist") +
  labs(
    title = "Prior Predictive Check: Losses",
    subtitle = "Model 1: Random Intercept Only (brms)"
  ) +
  theme_minimal()
```

A more pragmatic, weakly-informative prior set.

```{r}
#| label: fit-model-25b-brms
priors <- c(
  # Center on historical typical rate per $10k
    prior(student_t(3, log(100), 0.5), class = Intercept),
  # Between-class sd on log-rate ~ 0.3 (≈ exp(±0.6) ≈ ×[0.55, 1.82] for ±2σ as a rough guide)
    prior(student_t(3, 0, 0.3), class = sd, group = "Class", coef = "Intercept"),
  # Gamma shape prior as you had (CV ≈ 1/sqrt(shape), so median CV ≈ 0.55)
    prior(lognormal(log(3.3), 0.4), class = shape)
)

mod_0b_brms <- brms::brm(
    formula = bf(Loss ~ offset(log_payroll) + 1 + (1 | Class)),
    data = comp_loss,
    family = Gamma(link = "log"),
    prior = priors,
    sample_prior = "only",
    chains = 4,
    cores = 4,
    iter = 6000,
    threads = threading(2),
    seed = 648,
    backend = "rstan",
    refresh = 0,
    file = "Models/workcomp/_mod-0b-brms"
)
```

Check our new prior.

```{r}
#| label: mod-0-brms-ppc-weakly-informative
#| fig-cap: Prior Predictive Check: Losses
pp_check(mod_0b_brms, ndraws = 1, type = "hist") +
  labs(
    title = "Prior Predictive Check: Losses",
    subtitle = "Model 1: Random Intercept Only (brms)"
  ) +
  theme_minimal()
```

This seems like a more reasonable scale for losses.  Weakly informative but not overly diffuse.

```{r}
#| label: mod-0b-brms-rate-check
#| echo: false
# Includes offset by construction: E[y|θ] = payroll * rate
mu_draws  <- posterior_epred(mod_0b_brms)            # draws x obs, on response scale
rate_draws <- sweep(mu_draws, 2, comp_loss$Payroll_M, "/")  # rate = mu / payroll
rate_means <- colMeans(rate_draws)
rate_obs <- summary(comp_loss$Loss / comp_loss$Payroll_M)
print("Observed rates:")
print(rate_obs)
print("Prior predictive model rates:")
print(summary(rate_means))
```

These values are still relatively diffuse but don't violate our sense of **scale realism**.

Looking at the **variation realism** we would ideally consider the between-Class and within-Class spreads in the simulated data versus the observed data.  In this intercept-only model, all systematic differences between Classes arise from the random intercepts $u_j$. Conditional on those intercepts, the expected loss rate is identical for all observations within a Class. However, there is still stochastic within-Class variation in individual losses due to the Gamma sampling distribution.

Work on the **log-rate** scale where the model is additive.

$$
\log \text{rate}_{ij} = \beta_0 + u_j, \quad u_j \sim N(0, \sigma_{\text{Class}}^2)
$$

Class effects and spread per draw:

```{r}
#| label: mod-0b-brms-between-class-spread
#| echo: false
u_draws <- ranef(mod_0b_brms, summary = FALSE)$Class[, , "Intercept"] # matrix: ndraws x nclasses
sd_between_lograte <- u_draws |> 
  asplit(1) |>        # split matrix into list by rows
  purrr::map_dbl(sd)   # compute sd for each row, returning a numeric vector
print("Between-Class SD on log-rate scale (prior):")
print(quantile(sd_between_lograte, c(.05, .5, .95)))
print("Mean Between-Class SD on log-rate scale (prior):")
print(mean(sd_between_lograte))
```

Here we're drawing from the random intercepts for each Class from the prior.

$$
u_j \sim N(0, \sigma_{\text{Class}}^2)
$$

Then measure the **sample SD across all Classes** for each draw.  This gives us the distribution of the **between-Class spread on the log-rate scale** implied by our prior.  Under our prior, 90% of the time the realized between-Class will lie between 0.02 and 0.94 with a typical value areound 0.23.  If we look at the interval for $\exp(\pm 2 \times \text{SD})$ this gives us a sense of the multiplicative spread in rates across Classes.

- For log 0.02  → exp(±0.04) ≈ ×[0.96, 1.04] (very little spread between Classes)
- For log 0.23  → exp(±0.46) ≈ ×[0.63, 1.59] (moderate spread between Classes)
- For log 0.94  → exp(±1.88) ≈ ×[0.15, 6.55] (very wide spread between Classes, one class could be 6x another)

Verify this against the observed between-Class dispersion.

```{r}
#| label: mod-0b-brms-observed-between-class-spread
#| echo: false
obs_rate <- with(comp_loss, Loss / Payroll_M)
obs_class_means <- tapply(obs_rate, comp_loss$Class, mean, na.rm = TRUE)

# Spread across Classes on log-rate scale
sd_between_lograte_obs <- sd(log(obs_class_means), na.rm = TRUE)
print("Observed Between-Class SD on log-rate scale:")
print(sd_between_lograte_obs)
```

We observe the between-Class SD on log-rate scale is approximately 0.97.  On the rate scale, ±2 $\sigma$ covers exp(±1.94) ≈ ×[0.14, 6.96], which is very wide.  The prior typical spread was approximately 0.23 but our observed data show approximately 0.97, suggesting our prior may be too tight on the between-Class heterogeneity.

We'll change the prior to `prior(student_t(3, 0, 1.2), class = sd, group = "Class", coef = "Intercept")` to be more weakly informative.

Finally, we'll plot a prior predictive realsim check.

```{r}
#| label: mod-0b-brms-prior-predictive-class-means
#| fig-cap: Prior-predictive class means with observed overlay

# Prior predictive class mean rates (no obs noise)
eta <- posterior_linpred(mod_0b_brms, transform = FALSE) # includes offset
log_rate <- sweep(eta, 2, comp_loss$log_payroll, "-")    # remove offset
rate_draws <- exp(log_rate)

# collapse to class means per draw
idx <- split(seq_len(ncol(rate_draws)), comp_loss$Class)
rate_class_mean_draws <- sapply(idx, function(cols) rowMeans(rate_draws[, cols, drop = FALSE])) # draws x nclasses

# 90% PI and median per class
class_PI <- apply(rate_class_mean_draws, 2, quantile, probs = c(0.05, 0.5, 0.95))
# join observed class means
obs_cm <- obs_class_means[colnames(rate_class_mean_draws)]

class_levels <- colnames(rate_class_mean_draws)

df_plot <- tibble(
  Class = factor(class_levels, levels = class_levels),
  x     = seq_along(class_levels),
  q05   = class_PI[1, ],
  q50   = class_PI[2, ],
  q95   = class_PI[3, ],
  obs   = as.numeric(obs_cm[class_levels])
)

ggplot(df_plot, aes(x = x)) +
  geom_ribbon(aes(ymin = q05, ymax = q95), alpha = 0.20) +
  geom_line(aes(y = q50), linewidth = 0.7) +
  geom_point(aes(y = obs), size = 1.6) +
  scale_x_continuous(breaks = df_plot$x, labels = df_plot$Class, expand = c(0.01, 0.01)) +
  labs(
    title = "Prior-predictive class means with observed overlay",
    x = "Class",
    y = "Rate per $10k (prior predictive)"
  ) +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

We can see some of the observed class means well above the gray-band (our 90% prior predictive interval) and some near 0.  This suggests our prior is still too tight on the between-Class heterogeneity.

**What "between-Class" means in the intercept-only model**

$$
\log(\mu_{ij}) = \log(\text{Payroll}_{ij}) + \beta_0 + u_j, \quad u_j \sim N(0, \sigma_{\text{Class}}^2)
$$

- No covariates other than the offset
- All _systematic_ variation in expected rates across Classes comes from the random intercepts $u_j$
- The "between-Class" variation is entirely driven by the random intercepts $u_j$

**What "within-Class" means in the intercept-only model**

There is still within-Class variation, but it's purely _random noise_ from the Gamma distribution.

Each observation is $Y_{ij} \sim \text{Gamma}$ with $\text{Var}(Y_{ij}) = \frac{\mu_{ij}^2}{\text{shape}}$.

So even if all observations in a class share the same mean $\mu_{ij} = \mu_j$, the actual observed losses $Y_{ij}$ will vary around that mean due to the Gamma sampling variability.

Now fit the full model with the more weakly informative prior on the between-Class SD.

```{r}
#| label: fit-model-25c-brms
priors <- c(
  # Center on historical typical rate per $10k
    prior(student_t(3, log(100), 0.5), class = Intercept),
  # Between-class sd on (≈ exp(±1.94) ≈ ×[0.14, 6.96] for ±2σ as a rough guide - expanded to be a more weakly informative prior)
    prior(student_t(3, 0, 1.2), class = sd, group = "Class", coef = "Intercept"),
  # Gamma shape prior as you had (CV ≈ 1/sqrt(shape), so median CV ≈ 0.55)
    prior(lognormal(log(3.3), 0.4), class = shape)
)

mod_0c_brms <- brms::brm(
    formula = bf(Loss ~ offset(log_payroll) + 1 + (1 | Class)),
    data = comp_loss,
    family = Gamma(link = "log"),
    prior = priors,
    chains = 4,
    cores = 4,
    iter = 6000,
    threads = threading(2),
    seed = 648,
    backend = "rstan",
    refresh = 0,
    file = "Models/workcomp/_mod-0c-brms"
)
```

A quick check of the posterior predictive distribution versus observed data.  The posterior fit looks reasonable.

```{r}
#| label: mod-25c-brms-ppc
#| echo: false
#| fig-cap: Posterior Predictive Check: Loss Rate per $10k Payroll
obs_rate <- comp_loss$Loss / comp_loss$Payroll_M
K = 1000
yrep <- posterior_predict(mod_0c_brms, ndraws = 1000)
rate_rep <- sweep(yrep, 2, comp_loss$Payroll_M, "/")

set.seed(33)
sel <- sample(seq_len(K), 100)
p_overlay <- ppc_dens_overlay(y = obs_rate, yrep = rate_rep[sel, ])
p_overlay +
  labs(
    title = "Posterior Predictive Check: Loss Rate per $10k Payroll",
    subtitle = "Model 1: Random Intercept Only (brms)"
  ) +
  theme_minimal()
```

The marginal posterior predictive simulations for the marginal losses look reasonable compared to `glmmTMB` bootstrap simulations.

```{r}
#| label: mod-0-brms-marginal-simulations
#| echo: false
#| fig-cap: Posterior Predictive Simulations from Model 1 (marginalizing out class effects)

K <- 100  # draws per observation for plotting

# Simulated losses (with noise), marginal (exclude random effects):
yrep_marg <- posterior_predict(mod_0_brms, ndraws = K, re_formula = NA)  # K x N
# Fitted means (no noise), marginal:
mu_marg   <- posterior_epred(mod_0_brms, ndraws = K, re_formula = NA)    # K x N

index <- tibble(
  obs = seq_len(nrow(comp_loss)),
  Year = comp_loss$Year
)

# Simulated losses (marginal)

sim_long <- as_tibble(yrep_marg) |> 
  dplyr::mutate(draw = row_number()) |>
  tidyr::pivot_longer(cols = -draw, names_to = "obs", values_to = "loss") |> 
  dplyr::mutate(obs = as.integer(sub("^V", "", obs))) |>
  dplyr::left_join(index, by = "obs")

# Fitted means (marginal)

mu_long <- as_tibble(mu_marg) |> 
  dplyr::mutate(draw = row_number()) |>
  tidyr::pivot_longer(cols = -draw, names_to = "obs", values_to = "mu") |> 
  dplyr::mutate(obs = as.integer(sub("^V", "", obs))) |>
  dplyr::left_join(index, by = "obs")

# Aggregate fitted means by Year (marginal)
mu_year <- mu_long |> 
  dplyr::group_by(Year, draw) |> 
  dplyr::summarize(mu = median(mu), .groups = "drop")

marg_freq <- mod_0_tmb_sim_df |> 
  dplyr::select(Year, mu_marg) |>
  tidyr::pivot_longer(
    cols = c(mu_marg),
    names_to = "series",
    values_to = "loss"
  ) |> 
  dplyr::mutate(
    Year = factor(Year),
    series = factor(
      series,
      levels = c("mu_marg"),
      labels = c("freq_marg_fit")
    )
  )

sim_b <- sim_long |> 
  dplyr::group_by(obs, Year) |> 
  dplyr::summarize(loss = median(loss), .groups = "drop") |> 
  dplyr::select(-obs)
sim_b$series = "Bayes (ppc median)"
freq_b <- marg_freq |> dplyr::transmute(Year = factor(Year), loss, series = "Freq (marginal fit)")
freq_b$Year <- as.integer(as.character(freq_b$Year))

combined <- dplyr::bind_rows(
  sim_b |> dplyr::transmute(Year = factor(Year), loss, series),
  freq_b |> dplyr::transmute(Year = factor(Year), loss, series)
)

ggplot(combined, aes(x = factor(Year), y = loss, fill = series)) +
  geom_boxplot(position = position_dodge(width = 0.8), width = 0.35, alpha = 0.4, outlier.alpha = 0.15) +
  scale_y_log10() +
  labs(
    title = "Posterior Predictive Simulations (marginal): boxplots by Year",
    subtitle = "Model 1: Random Intercept Only (brms)",
    x = "Year",
    y = "log Loss (marginal)"
  ) +
  theme_minimal()
```

```{r}
#| label: mod-0-brms-conditional-simulations
#| echo: false
#| fig-cap: Posterior Predictive Simulations from Model 1 (including class effects)

classes_to_show <- c("10","43")  # adjust to your class labels (must match factor levels)
K <- 100

# Conditional simulations (include random effects by default):
yrep_cond <- posterior_predict(mod_0_brms, ndraws = K, seed = 646)
mu_cond   <- posterior_epred(mod_0_brms, ndraws = K, seed = 646)

index2 <- tibble(
  obs   = seq_len(nrow(comp_loss)),
  Year  = comp_loss$Year,
  Class = comp_loss$Class
) |>
  filter(Class %in% classes_to_show)

# Long: simulated losses, restricted to chosen classes
sim_cond <- as_tibble(yrep_cond) |> 
  dplyr::mutate(draw = row_number()) |> 
  tidyr::pivot_longer(cols = -draw, names_to = "obs", values_to = "loss") |> 
  dplyr::mutate(obs = as.integer(readr::parse_number(obs))) |> 
  dplyr::inner_join(index2, by = "obs")

# Long: fitted means, restricted
mu_cond_long <- as_tibble(mu_cond) |> 
  dplyr::mutate(draw = row_number()) |> 
  tidyr::pivot_longer(cols = -draw, names_to = "obs", values_to = "mu") |> 
  dplyr::mutate(obs = as.integer(readr::parse_number(obs))) |> 
  dplyr::inner_join(index2, by = "obs")

mu_by_year_class <- mu_cond_long |> 
  dplyr::group_by(Class, Year, draw) |> 
  dplyr::summarize(mu = median(mu), .groups = "drop")

ggplot2::ggplot(data = sim_cond, aes(x = factor(Year), y = loss)) +
  geom_boxplot(fill = "grey80", width = 0.6, outlier.alpha = 0.15) +
  geom_point(data = comp_loss |> filter(Class %in% classes_to_show),
             aes(x = factor(Year), y = Loss),
             inherit.aes = FALSE, size = 0.6, alpha = 0.25) +
  geom_line(data = mu_by_year_class,
            aes(y = mu, group = draw), color = "red", linewidth = 0.6, alpha = 0.1) +
  scale_y_log10() +
  facet_wrap(~ Class, ncol = length(classes_to_show), scales = "free_y") +
  labs(
    title = "Posterior Predictive Simulations (conditional): boxplots by Year",
    subtitle = paste0("Model 1: Random Intercept Only (brms); K = ", K, " draws per observation; red lines = fitted means"),
    x = "Year",
    y = "log Loss (conditional)"
  ) +
  theme_minimal()
```

- We can see similar means between the `glmmTMB` and `brms` models for the conditional fits → the two fits agree on systematic effects.
- However, both models fail to reliably capture the upward time trend seen in the data because neither model includes a time trend.
- We can see the `brms` model naturally integrates parameter uncertainty into the fitted means.
- The `brms` model is a richer, more honest reflection of model uncertainty.

### Model 2 - Adding a Time Trend

$\log \left(\mu_{i j}\right) =\log \left(\mathrm{PAYROLL}_{i j}\right)+\beta_{0}+\beta_{1} \mathrm{Year}_{i j}+b_{i, 0}$

Fixed effect on Year (centered) and random intercept.

```{r}
#| label: model-26-glmmTMB
mod_1_tmb <- glmmTMB(
    formula = Loss ~ 1 + Year_C + (1 | Class),
    data = comp_loss,
    family = Gamma(link = "log"),
    offset = log(comp_loss$Payroll_M),
    REML = TRUE
)
```

```{r}
#| label: model-26-glmmTMB-beta1-estimate
#| echo: false
summ <- summary(mod_1_tmb)
b1   <- summ$coefficients$cond["Year_C","Estimate"]
se1  <- summ$coefficients$cond["Year_C","Std. Error"]

c(estimate = b1,
  annual_multiplier = exp(b1),             # multiplicative change per +1 in Year_C
  pct_per_year = 100*(exp(b1)-1),          # % change per year
  ci_low  = exp(b1 - 1.96*se1),
  ci_high = exp(b1 + 1.96*se1))
```

The coefficient on `Year_C` (β₁ ≈ 3 × 10⁻⁵, 95 % CI [−0.02, 0.02]) is indistinguishable from zero, indicating no evidence of a temporal trend in the fitted loss rates.

$$
\widehat{\beta_1}=3 \times 10^{-5} \Rightarrow 0.003 \% / \text{yr}
$$

```{r}
#| label: predictive-boxplot-mod_1_tmb
#| echo: false
#| fig-cap: Predictive Boxplots from Model 2 (glmmTMB)
predictive_boxplot(mod_1_tmb, comp_loss,
                   group_var = "Year_C",
                   class_var = "Class",
                   classes = c("10","43"),
                   K = 100,
                   conditional = TRUE,
                   as_rate = TRUE)
```

**brms model formulation**

```{r}
#| label: model-26-brms-priors
#| echo: false
brms::get_prior(
    formula = bf(Loss ~ offset(log_payroll) + 1 + Year_C + (1 | Class)),
    data = comp_loss,
    family = Gamma(link = "log")
)
```

Now we have two types of fixed effects, Intercept and Year_C.

YearC is a slope for time trend, with class `b`, where `class = b` corresponds to a non-intercept regression coefficient(s).  `(flat)` means a **non-informative, uniform prior** over the real line.  Formally, this is an _improper_ prior:

$$
p(\beta) \propto 1, \quad-\infty<\beta<\infty
$$

So there's **no regularization**, uniform over $\mathbb{R}$.  Ideally, we don't want improper priors generally, or flat priors on slopes.  Usual best practice is to **regularize slope coefficients** with weakly informative priors.

`prior(normal(0, 0.1), class = b, coef = Year)`  This prior can constrain the model toward a modest annual trend unless there is compelling data otherwise.

Per **one unit** of `Year_C` (i.e., one year), the multiplicative change in rate is $\exp(\beta_1)$.  With a prior of $\beta_1 \sim N(0, 0.1^2)$, a 95% prior band is roughly $\beta_1 \in [-0.2, 0.2]$, i.e., $\exp(\beta_1) \in [0.82, 1.22]$ or roughly ±20% per unit.

```{r}
#| label: fit-model-26-brms
priors <- c(
  # Center on historical typical rate per $10k
    prior(student_t(3, log(100), 0.5), class = Intercept),
    prior(normal(0, 0.1), class = b, coef = Year_C),
  # Between-class sd on (≈ exp(±1.94) ≈ ×[0.14, 6.96] for ±2σ as a rough guide - expanded to be a more weakly informative prior)
    prior(student_t(3, 0, 1.2), class = sd, group = "Class", coef = "Intercept"),
  # Gamma shape prior as you had (CV ≈ 1/sqrt(shape), so median CV ≈ 0.55)
    prior(lognormal(log(3.3), 0.4), class = shape)
)

mod_1_brms <- brms::brm(
    formula = bf(Loss ~ offset(log_payroll) + 1 + Year_C + (1 | Class)),
    data = comp_loss,
    family = Gamma(link = "log"),
    prior = priors,
    sample_prior = "only",
    chains = 4,
    cores = 4,
    warmup = 1000,
    iter = 5000,
    threads = threading(2),
    seed = 1012,
    backend = "rstan",
    refresh = 0,
    file = "Models/workcomp/_mod-1-brms"
)
```

Check on the reasonableness of this prior.

```{r}
#| label: mod-26-brms-ppc-prior

draws <- brms::as_draws_df(mod_1_brms)
summary_beta1 <- posterior::summarise_draws(draws) |> 
  dplyr::filter(variable == "b_Year_C")
summary_beta1
```

```{r}
# per-draw multiplicative change for +1 in Year_C
beta1 <- as.numeric(draws$b_Year_C)
summary(exp(beta1))
quantile(exp(beta1), c(.05, .5, .95))
```

Population-level distribution by Year_C group seems reasonable.

```{r}
#| label: mod-26-brms-ppc-prior-check
#| fig-cap: Prior Predictive Check: Losses by Year_C
pp_check(mod_1_brms, ndraws = 100, type = "stat_grouped",
         stat = "mean", group = "Year_C", re_formula = NA)
```

Now fit the full model with data.

```{r}
#| label: fit-model-26b-brms
priors <- c(
  # Center on historical typical rate per $10k
    prior(student_t(3, log(100), 0.5), class = Intercept),
    prior(normal(0, 0.1), class = b, coef = Year_C),
  # Between-class sd on (≈ exp(±1.94) ≈ ×[0.14, 6.96] for ±2σ as a rough guide - expanded to be a more weakly informative prior)
    prior(student_t(3, 0, 1.2), class = sd, group = "Class", coef = "Intercept"),
  # Gamma shape prior as you had (CV ≈ 1/sqrt(shape), so median CV ≈ 0.55)
    prior(lognormal(log(3.3), 0.4), class = shape)
)

mod_1b_brms <- brms::brm(
    formula = bf(Loss ~ offset(log_payroll) + 1 + Year_C + (1 | Class)),
    data = comp_loss,
    family = Gamma(link = "log"),
    prior = priors,
    chains = 4,
    cores = 4,
    warmup = 1000,
    iter = 5000,
    threads = threading(2),
    seed = 1032,
    backend = "rstan",
    refresh = 0,
    file = "Models/workcomp/_mod-1b-brms"
)
```

```{r}
#| label: mod-26b-brms-ppc
#| echo: false
#| fig-cap: Posterior Predictive Check: Loss Rate per $10k Payroll
obs_rate <- comp_loss$Loss / comp_loss$Payroll_M
K = 1000
yrep <- posterior_predict(mod_1b_brms, ndraws = 1000)
rate_rep <- sweep(yrep, 2, comp_loss$Payroll_M, "/")

set.seed(33)
sel <- sample(seq_len(K), 100)
p_overlay <- ppc_dens_overlay(y = obs_rate, yrep = rate_rep[sel, ])
p_overlay +
  labs(
    title = "Posterior Predictive Check: Loss Rate per $10k Payroll",
    subtitle = "Model 2: Year_C Fixed Effect + Random Intercept (brms)"
  ) +
  theme_minimal()
```

We can't say that the time trend is different from zero for the population average.

```{r}
#| label: mod-26b-brms-beta1-posterior
#| echo: false
#| fig-cap: Posterior Distribution for β₁ (Year_C)
beta1_mat <- as.matrix(mod_1b_brms, variable = "b_Year_C")
dim(beta1_mat)
bayesplot::mcmc_areas(beta1_mat, prob = 0.8, prob_outer = 0.95) +
  labs(
    title = "Posterior Distribution for β₁ (Year_C)",
    x = "β₁ (log-rate scale)",
    y = ""
  ) +
  theme_minimal()
```

```{r}
#| label: predictive-boxplot-mod_1_brms
#| echo: false
#| fig-cap: Predictive Boxplots from Model 2 (brms)
predictive_boxplot(mod_1b_brms, comp_loss,
                   group_var = "Year_C",
                   class_var = "Class",
                   classes = c("10","43"),
                   K = 100,
                   conditional = TRUE,
                   as_rate = TRUE)
```

### Model 3 - Adding a Random Time Trend per Class

$\log \left(\mu_{i j}\right) =\log \left(\mathrm{PAYROLL}_{i j}\right)+\beta_{0}+\beta_{1} \mathrm{Year}_{i j}+b_{i, 0}+b_{i, 1} \mathrm{Year}_{i j}$

Fixed effects Intercept and Slope and Random Intercept and Slope

The model:

$$
\log \left(\mu_{i j}\right)=\log \left(\text{Payroll}_{i j}\right)+\left(\beta_0+u_{0 j}\right)+\left(\beta_1+u_{1 j}\right) \text{Year}_C .
$$

- $\beta_0, \beta_1$ are the population-level intercept and slope - "average" trend across all Classes.
- $u_{0 j}, u_{1 j}$ are the Class-specific deviations from the population-level intercept and slope.
- Each class $j$ has its own fitted line:

$$
\log \left(\text{rate}_j\right)=\left(\beta_0 + u_{0 j} \right) + \left(\beta_1 + u_{1 j} \right) \text{Year}_C
$$


```{r}
#| label: model-27-glmmTMB
mod_2_tmb <- glmmTMB(
    formula = Loss ~ 1 + Year_C + (1 + Year_C | Class),
    data = comp_loss,
    family = Gamma(link = "log"),
    offset = log(comp_loss$Payroll_M),
    REML = TRUE
)
```

```{r}
#| label: predictive-boxplot-mod_2_tmb
#| echo: false
#| fig-cap: Predictive Boxplots from Model 3 (glmmTMB)
predictive_boxplot(mod_2_tmb, comp_loss,
                   group_var = "Year_C",
                   class_var = "Class",
                   classes = c("10","43"),
                   K = 100,
                   conditional = TRUE,
                   as_rate = TRUE)
```

- The red line is that class's fitted **conditional mean rate**
$$
\hat{\operatorname{rate}}_{j, t}=\exp \left(\beta_0+u_{0 j}+\left(\beta_1+u_{1 j}\right) \operatorname{Year}_C(t)\right)
$$

- The boxplots show the posterior predictive distribution for losses in that year, including both the uncertainty in the mean rate and the Gamma sampling variability.
- We're seeing the **full predictive unvertainty** within each class.

**brms model formulation**

```{r}
#| label: brms-get-prior-model-27
#| echo: false
brms::get_prior(
    formula = bf(Loss ~ offset(log_payroll) + 1 + Year_C + (1 + Year_C | Class)),
    data = comp_loss,
    family = Gamma(link = "log")
)
```

We need a prior on the correlation matrix of our random effects, i.e., on the **off-diagonal structure** of $\Sigma$.

$$
\binom{u_{0 j}}{u_{1 j}} \sim \mathcal{N}\left(\binom{0}{0}, \Sigma\right), \quad \Sigma=\left(\begin{array}{cc}
\sigma_0^2 & \rho \sigma_0 \sigma_1 \\
\rho \sigma_0 \sigma_1 & \sigma_1^2
\end{array}\right)
$$

The **LKJ (Lewandowski-Kurowicka-Joe)** prior is a probability distribution over _correlation matrices_ that ensures they are valid (positive-definite, unit diagonals).

`lkj(1)` means $\operatorname{Corr}(\Sigma) \sim \operatorname{LKJ}(\eta=1)$ where $\eta$ controls how strongly the distribution favors correlations near 0.

| η value | Description                                                                        | Shape of correlation prior (for ρ in 2×2 case) |
| ------- | ---------------------------------------------------------------------------------- | ---------------------------------------------- |
| η = 1   | **Uniform** over all valid correlation matrices — all correlations equally likely. | Flat between −1 and +1                         |
| η > 1   | **Concentrates near 0** (prefers low correlations).                                | Bell-shaped around 0                           |
| η < 1   | **Concentrates toward ±1** (prefers extreme correlations).                         | U-shaped                                       |

If you wanted to mildly regularize (discourage extreme correlations unless data demand it), you can use:

`prior(lkj(2), class = cor)`

For our model, the random effects `(1 + Year_C | Class)` the covariance matrix is

$$
\Sigma=\left(\begin{array}{cc}
\sigma_{\text {Intercept }}^2 & \rho \sigma_{\text {Intercept }} \sigma_{\text {Year }} \\
\rho \sigma_{\text {Intercept }} \sigma_{\text {Year }} & \sigma_{\text {Year }}^2
\end{array}\right)
$$

For the 2x2 case, the LKJ prior has pdf:

$$
p(\rho) \propto \left(1 - \rho^2 \right)^{\eta-1}
$$

Hence:

- for $\eta = 1 \rightarrow p(\rho) \propto 1$ (flat on [-1, 1])
- for $\eta > 1 \rightarrow p(\rho) \propto (1-\rho^2)$ (central correlations more likely)
- for $\eta < 1 \rightarrow p(\rho) \propto (1-\rho^2)^{-0.5}$ (more mass at -1 and 1)

```{r}
#| label: lkj-correlation-prior-visualization
#| echo: false
#| fig-cap: LKJ Prior Distributions for Correlation Matrices (2x2 case)
library(ggdist)
library(ggtext)
tidyr::tibble(eta = c(0.2, 1, 5, 15)) |> 
  dplyr::mutate(prior = glue::glue("lkjcorr({eta})")) |>
  dplyr::mutate(prior_nice = fct_inorder(glue::glue("LKJ(η = {eta})"))) |> 
  parse_dist(prior) |> 
  ggdist::marginalize_lkjcorr(K = 2) |>  # K = dimension of correlation matrix; ours is 2x2 here
  ggplot(aes(y = 0, dist = .dist, args = .args)) +
    stat_slab(fill = "lightblue") +
    labs(x = "") +
    theme(axis.title.x = element_markdown(), axis.text.y = element_blank(), 
            axis.title.y = element_blank(), axis.ticks.y = element_blank(),
            panel.grid = element_blank()) +
    theme_minimal() +
    facet_wrap(vars(prior_nice), nrow = 1)
```

Given our new prior structure, let's fit the model with priors only to check our priors.

```{r}
#| label: fit-model-27-brms-prior
priors <- c(
  # fixed effects
    prior(student_t(3, log(100), 0.5), class = Intercept),
    prior(normal(0, 0.1), class = b, coef = "Year_C"),

  # group-level SDs (log-rate scale)
    prior(student_t(3, 0, 1.2), class = sd, group = "Class", coef = "Intercept"),
    prior(student_t(3, 0, 0.6), class = sd, group = "Class", coef = "Year_C"),

  # correlation between random effects
    prior(lkj(2), class = cor, group = "Class"),

  # Gamma shape
    prior(lognormal(log(3.3), 0.4), class = shape)
)

mod_2_brms_prior <- brms::brm(
    formula = bf(Loss ~ offset(log_payroll) + 1 + Year_C + (1 + Year_C | Class)),
    data = comp_loss,
    family = Gamma(link = "log"),
    prior = priors,
    sample_prior = "only",
    chains = 4,
    cores = 4,
    warmup = 1000,
    iter = 5000,
    threads = threading(2),
    seed = 1739,
    backend = "rstan",
    refresh = 0,
    file = "Models/workcomp/_mod-2-brms-prior"
)
```

**Visualizing the prior over the correlation (LKJ)**

Our prior on the correlation between intercept and slope within Class looks reasonable.

```{r}
#| label: lkj-correlation-prior-visualization-mod-2
#| echo: false
#| fig-cap: Prior Distribution for corr(Intercept, Year_C) within Class (LKJ(2))
draws_prior <- brms::as_draws_df(mod_2_brms_prior)

# Find correlation parameter name(s)
param <- grep("^cor_Class__Intercept__Year_C$", names(draws_prior), value = TRUE)

ggplot(draws_prior, aes(x = .data[[param]])) +
    geom_histogram(aes(y = after_stat(density)), 
                   bins = 50, boundary = 0, closed = "left", fill = "lightblue") +
    geom_density(linewidth = 0.8) +
    labs(
        title = "LKJ(2) prior on corr(Intercept, Year_C) withing Class",
        x = "Correlation ρ", y = "Density"
    ) +
    theme_minimal()
```

Now fit the full model with data.

```{r}
#| label: fit-model-27-brms
priors <- c(
  # fixed effects
    prior(student_t(3, log(100), 0.5), class = Intercept),
    prior(normal(0, 0.1), class = b, coef = "Year_C"),

  # group-level SDs (log-rate scale)
    prior(student_t(3, 0, 1.2), class = sd, group = "Class", coef = "Intercept"),
    prior(student_t(3, 0, 0.6), class = sd, group = "Class", coef = "Year_C"),

  # correlation between random effects
    prior(lkj(2), class = cor, group = "Class"),

  # Gamma shape
    prior(lognormal(log(3.3), 0.4), class = shape)
)

mod_2_brms <- brms::brm(
    formula = bf(Loss ~ offset(log_payroll) + 1 + Year_C + (1 + Year_C | Class)),
    data = comp_loss,
    family = Gamma(link = "log"),
    prior = priors,
    chains = 4,
    cores = 4,
    warmup = 1500,
    iter = 10000,
    threads = threading(2),
    seed = 1821,
    backend = "rstan",
    refresh = 0,
    control = list(adapt_delta = 0.95, max_treedepth = 12),
    save_pars = brms::save_pars(all = TRUE),
    file = "Models/workcomp/_mod-2-brms"
)
```

Posterior predictive check looks reasonable.

```{r}
#| label: mod-27b-brms-ppc
#| echo: false
#| fig-cap: Posterior Predictive Check: Loss Rate per $10k Payroll
obs_rate <- comp_loss$Loss / comp_loss$Payroll_M
K = 1000
yrep <- posterior_predict(mod_2_brms, ndraws = 1000)
rate_rep <- sweep(yrep, 2, comp_loss$Payroll_M, "/")

set.seed(12)
sel <- sample(seq_len(K), 100)
p_overlay <- ppc_dens_overlay(y = obs_rate, yrep = rate_rep[sel, ])
p_overlay +
  labs(
    title = "Posterior Predictive Check: Loss Rate per $10k Payroll",
    subtitle = "Model 2: Year_C Fixed Effect + Random Intercept (brms)"
  ) +
  theme_minimal()
```

**Visualizing the posterior correlation**

```{r}
#| label: lkj-correlation-posterior-visualization-mod-2
#| fig-cap: Posterior Distribution for corr(Intercept, Year_C) within Class
draws_post <- brms::as_draws_df(mod_2_brms)
param <- grep("^cor_Class__Intercept__Year_C$", names(draws_post), value = TRUE)

# Posterior density + interval
ggplot(draws_post, aes(x = .data[[param]])) +
  stat_halfeye(fill = "lightblue") +
  geom_vline(xintercept = 0, linetype = 2) +
  labs(
    title = "Posterior for corr(Intercept, Year_C) within Class",
    x = "Correlation ρ", y = NULL,
    subtitle = "stat_halfeye shows density + median + HDI"
  ) +
  theme_minimal()
```

Here the red lines refelct the ...

```{r}
#| label: predictive-boxplot-mod_2_brms
#| echo: false
#| fig-cap: Predictive Boxplots from Model 2 (brms)
predictive_boxplot(mod_2_brms, comp_loss,
                   group_var = "Year_C",
                   class_var = "Class",
                   classes = c("10","43"),
                   K = 100,
                   conditional = TRUE,
                   as_rate = TRUE)
```

**Posterior predictive checks on Random Slopes**

```{r}
as_draws_df(mod_2_brms) |>
  ggplot(aes(x = b_Year_C)) +
  stat_halfeye(.width = c(.8,.95)) +
  geom_vline(xintercept = 0, linetype = 2)

```

```{r}
library(tidybayes)

target <- "43"  # the class you want

# u1_j = random-slope deviation for that class
draws_u1 <- mod_2_brms %>%
  spread_draws(r_Class[Class, Year_C]) %>%   # all classes' u1 draws
  filter(Class == target)

ggplot(draws_u1, aes(x = r_Class)) +
  stat_halfeye(.width = c(.8, .95)) +
  geom_vline(xintercept = 0, linetype = 2) +
  labs(title = paste("Random slope u₁ for Class", target),
       x = "u₁ (deviation on log-rate scale)")

```

```{r}
draws_beta1_class <- mod_2_brms %>%
  spread_draws(b_Year_C, r_Class[Class, Year_C]) %>%
  filter(Class == target) %>%
  mutate(beta1_class = b_Year_C + r_Class)

ggplot(draws_beta1_class, aes(x = beta1_class)) +
  stat_halfeye(.width = c(.8, .95)) +
  geom_vline(xintercept = 0, linetype = 2) +
  labs(title = paste("Slope for Class", target),
       x = "β₁ + u₁ (log-rate per +1 in Year_C)")
```

```{r}
# Full draws for random effects
re <- ranef(mod_2_brms, summary = FALSE)$Class    # iter x J x coef
dimnames(re)[[3]]                               # check coef names, e.g. "(Intercept)", "Year_C"

target <- "10"
j <- which(levels(comp_loss$Class) == target)

u1_draws <- re[, j, "Year_C"]                   # vector of draws for u1_j
hist(u1_draws, breaks = 40,
     main = paste("u₁ for Class", target), xlab = "u₁ (log-rate)")
abline(v = 0, lty = 2, col = "red")
```

```{r}
# Full draws for random effects
re <- ranef(mod_2_brms, summary = FALSE)$Class    # iter x J x coef
dimnames(re)[[3]]                               # check coef names, e.g. "(Intercept)", "Year_C"

target <- "43"
j <- which(levels(comp_loss$Class) == target)

u1_draws <- re[, j, "Year_C"]                   # vector of draws for u1_j
hist(u1_draws, breaks = 40,
     main = paste("u₁ for Class", target), xlab = "u₁ (log-rate)")
abline(v = 0, lty = 2, col = "red")
```

```{r}
re_cls <- ranef(mod_2_tmb)$cond$Class
blups <- re_cls |>
  rownames_to_column("Class") |>
  rename(b_intercept = `(Intercept)`)

beta0 <- fixef(mod_2_tmb)$cond["(Intercept)"]

coef_by_class <- blups |>
  mutate(
    intercept_log = beta0 + b_intercept,     # on log scale
    rate_per_payroll = exp(intercept_log)    # pure premium per $1 Payroll
  )

ggplot(blups, aes(x = reorder(Class, b_intercept), y = b_intercept)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point() +
  coord_flip() +
  labs(x = "Class", y = "Random intercept (log scale)",
       title = "Class-level BLUPs (random intercepts)")
```

### Posterior Predictive Calibration

#### Check LOO-PIT and Q-Q plot

A **bayesian model** claims to describe how the data are generated.  For every observation $y_i$, the model imples a _predictive distribution_

$$
p(\bar{y} \mid y)=\int p(\bar{y} \mid \theta) p(\theta \mid y) d \theta
$$

**Calibration** asks: _Do the real observations look like plausible draws from this predictive distribution?_

If the model is well calibrated:
- Observed values should look "typical" under their predictive distributions.
- About 80% of observations should fall in the 80% posterior predictive intervals, 95% in the 95% intervals, etc.
- The model should not systematically over- or under-predict certain regions of the data.
- This is a **model adquacy** or **goodness-of-fit** test.

**PIT (Probability Integral Transform)**

For any continuous variable $Y$ with CDF $F(y)$, if we compute $u=F(Y)$, then $u$ is uniformly distributed on [0, 1], _if and only if_ $F$ is the true distribution of $Y$.
- if $u$ are skewed left, we're under-predicting (observations larger than predicted)
- if $u$ are skewed right, we're over-predicting (observations smaller than predicted)
- if $u$ have spikes or troughs, the model is mis-shaped.

In the bayesian case, $F(y_i)$ is not a single fixed CDF by an _average predictive CDF_ intergrating over posterior uncertainty in parameters.

For each data point $i$, we compute:

$$
u_i = P\left(Y^{\text{rep}}_i \leq y_i \mid \text{data}\right)
$$

If our model describes the data-generating process correctly, then $u_i \sim \text{Uniform}(0,1)$.

**LOO-PIT (Leave-One-Out Probability Integral Transform)**

In hierarchical models, each $y_i$ depend on group-level parameters that are informed by $y_i$ itself.  This can lead to over-optimistic calibration assessments.  We're testing on the same information the model already saw.

For each observation $y_i$:

1. Leave out $y_i$ from the fitting data.
1. Recompute the posterior of parameters $\theta$ using the remaining $n-1$ data.
1. Compute the predictive distribution $p\left(y_i \mid \text{data}_{-i}\right)$.
1. Compute the PIT value:

$$
u_i = P\left(Y^{\text{rep}}_i \leq y_i \mid \text{data}_{-i}\right)
$$

(See Pareto-smoothed importance sampling in Vehtari et al. 2017 for efficient computation.)

If the model is well calibrated, then the $u_i$ should be i.i.d. $\text{Uniform}(0,1)$.

$$
\operatorname{Loss}_{i j} \sim \operatorname{Gamma}\left(\text{shape}, \mu_{i j}\right)
$$

with
$$
\log \left(\mu_{i j}\right)=\log \left(\text{Payroll}_{i j}\right)+\beta_0+\beta_1 \text{Year}_{i j}+u_{0 j}+u_{1 j} \text{Year}_{i j}
$$

- The priors fit the prior predictive distribution reasonably well.
- Now, _given these priors and the data, does the model reproduce the data's distributional shape, mean-variance relationship, and skewness golbally?_  This is what LOO-PIT aims to assess.

If the LOO-PIT looks uniform:
- The Gamma shape parameter & random effects together describe the heteroscedasticity and skewness adequately.
- The link function and offset properly adjust for scale.
- No systematic misfit across the support of `Loss`.
- If the LOO-PIT deviates from uniformity, the model is mis-calibrated globally in some way; we might need a different family (the variance structure is wrong), a more flexible model (e.g., mixture model), or additional predictors to explain systematic bias.

**Visualizing LOO-PIT**

_"Do the residuals look like noise?"_

It doesn't tell us _why_ the model is mis-calibrated, but it does tell us whether the model's global predictive structure is trustworthy.

```{r}
#| label: mod-27-brms-loo-pit-overlay

# compute LOO with momemt matching for large-k points
loo_mm   <- brms::loo(mod_2_brms, moment_match = TRUE, k_threshold = 0.7)

print(loo_mm)
table(loo_mm$diagnostics$pareto_k > 0.7, useNA = "ifany")
```

Interpretation of k diagnostics:
- k ≤ 0.5: Good, reliable PSIS-LOO approximation.
- 0.5 < k ≤ 0.7: OK, but be cautious.
- k > 0.7: Problematic, PSIS-LOO may be unreliable; the observation's log-likelihood has too much influence

We have 4 points exceeding 0.7, these correspond to a few outlying our overly influential claims or classes that have disproportionate leverage on the posterior.  In hierarchical models, this often means some Class groups may have very small exposure or highly unusual losses relative to their fitted mean.

```{r}
#| label: mod-27-brms-loo-high-k-points
which(loo_mm$diagnostics$pareto_k > 0.7)
```

```{r}
#| label: mod-27-brms-loo-pit-plot
#| fig-cap: LOO-PIT Plot for Model 3
plot(loo_mm)
```

Compare the empirical distribution of the LOO-PIT values to the ideal uniform distribution.  If the PIT is flat the model is globally well calibrated.  Here, we may be showing some systematic mis-calibration.

We may be predicting the center of the distribution well but underestimating the tails in how extreme some losses can be.  A simple Gamma with one shape paramater per model may not be flexible enough to capture the full range of loss variability.

```{r}
#| label: mod-27-brms-loo-pit-overlay-plot
#| fig-cap: LOO-PIT Overlay Plot for Model 3
pp_check(mod_2_brms, type = "loo_pit_overlay") +
  labs(
    title = "LOO-PIT Overlay Plot for Model 3",
  ) +
  theme_minimal()
```

We're still getting some high k values → problematic points for PSIS-LOO approximation.  Switching to `reloo = TRUE` will re-fit the model without each problematic observation to get exact LOO for those points.

```{r}
#| label: mod-27-brms-reloo

loo_rl <- brms::loo(mod_2_brms, reloo = TRUE, k_threshold = 0.7, save_psis = TRUE)
print(loo_rl)
```


```{r}
# y = observed vector; yrep = posterior predictive draws (matrix: ndraws x nobs)
y    <- comp_loss$Loss
yrep <- posterior_predict(mod_2_brms)

lw_rl <- weights(loo_rl$psis_object, normalize = TRUE)

ppc_loo_pit_qq(
  y = y,
  yrep = yrep,
  lw = lw_rl
) + theme_minimal()
```

#### Density Overlay of y vs yrep (global shape)

```{r}
#| label: mod-27-brms-ppc-density-overlay
#| fig-cap: Posterior Predictive Density Overlay for Model 3
pp_check(mod_2_brms, type = "dens_overlay", ndraws = 200) +
  labs(
    title = "Posterior Predictive Density Overlay for Model 3",
  ) +
  theme_minimal()
```

#### PPC for low-order moments

Ensure the model reproduces first/second order structure.

```{r}

ppc_mean <- ppc_stat(y, yrep = posterior_predict(mod_2_brms, ndraws = 1000), stat = "mean") +
  theme_minimal()
ppc_sd <- ppc_stat(y, yrep = posterior_predict(mod_2_brms, ndraws = 1000), stat = "sd") +
  theme_minimal()

ppc_mean + ppc_sd +
  plot_layout(ncol = 2) +
  plot_annotation(
    title = "Posterior Predictive Checks for Low-Order Moments"
  )
```

### Conditional Fits (by predictor and group)

#### Conditional PPC by `Year_C` (binned)

Compare observed vs replicated distributions along the covariate.

```{r}
#| label: mod-27-brms-ppc-stat-grouped

obs_rate <- comp_loss$Loss / comp_loss$Payroll_M
yrep <- posterior_predict(mod_2_brms, ndraws = 200)
rate_rep <- sweep(yrep, 2, comp_loss$Payroll_M, "/")

ppc_dens_overlay_grouped(obs_rate, rate_rep, group = factor(comp_loss$Year_C), alpha = 0.7)
```

#### Grouped coverage of predictive intervals by `Class`

Empirical coverage of PI, per Class.

```{r}

post <- add_predicted_draws(
  comp_loss,
  mod_2_brms,
  ndraws = 200,
  re_formula = NULL
)
```
```{r}
cov_df <- post |> 
  dplyr::group_by(Class, .row) |> 
  dplyr::summarise(
    obs = first(Loss),
    lo = quantile(.prediction, 0.1),
    hi = quantile(.prediction, 0.9),
    .groups = "drop"
  ) |> 
  dplyr::mutate(covered = as.integer(obs >= lo & obs <= hi)) |> 
  dplyr::group_by(Class) |>
  dplyr::summarise(cov80 = mean(covered), .groups = "drop")

ggplot(cov_df, aes(x = reorder(Class, cov80), y = cov80)) +
  geom_hline(yintercept = 0.8, linetype = 2, color = "red") +
  geom_point() +
  coord_flip() +
  theme_minimal()
```

```{r}


# 1) Build the 0/1 indicator for "covered by 80% PI"
post <- tidybayes::add_predicted_draws(comp_loss, mod_2_brms, ndraws = 800)

by_obs <- post |>
  group_by(Class, .row) |>
  summarise(
    obs = first(Loss),
    lo  = quantile(.prediction, 0.10),
    hi  = quantile(.prediction, 0.90),
    .groups = "drop"
  ) |>
  mutate(covered = as.integer(obs >= lo & obs <= hi))

# 2) Summarize to class-level and compute Jeffreys intervals
by_class <- by_obs |>
  group_by(Class) |>
  summarise(
    n   = n(),
    k   = sum(covered),
    p   = k / n,
    # Jeffreys posterior Beta(k+0.5, n-k+0.5)
    lo80 = qbeta(0.10, k + 0.5, n - k + 0.5),
    hi80 = qbeta(0.90, k + 0.5, n - k + 0.5),
    lo95 = qbeta(0.025, k + 0.5, n - k + 0.5),
    hi95 = qbeta(0.975, k + 0.5, n - k + 0.5),
    .groups = "drop"
  )

# 3) Plot: thick bar = 80% CI, thin whisker = 95% CI, point = p_hat
ggplot(by_class, aes(x = p, y = reorder(Class, p))) +
  # 95% CI (thin)
  geom_errorbarh(aes(xmin = lo95, xmax = hi95), height = 0.2, linewidth = 0.25, alpha = 0.6) +
  # 80% CI (thick)
  geom_errorbarh(aes(xmin = lo80, xmax = hi80), height = 0.35, linewidth = 1.1) +
  geom_point(size = 1.4) +
  geom_vline(xintercept = 0.80, linetype = 2, color = "red") +
  labs(x = "Empirical coverage of 80% predictive interval",
       y = "Class",
       title = "Class-level calibration: coverage with 80% & 95% binomial intervals") +
  coord_cartesian(xlim = c(0, 1)) +
  theme_bw()

```

# by_class must have columns: Class, n, p  (from the earlier code)

```{r}
# Helper to build Wilson bands around nominal p0
wilson_band <- function(n, p0 = 0.80, level = 0.95) {
  z <- qnorm(1 - (1 - level)/2)
  denom  <- 1 + z^2 / n
  center <- (p0 + (z^2/(2*n))) / denom
  half   <- (z * sqrt(p0*(1-p0)/n + z^2/(4*n^2))) / denom
  tibble(n = n, lo = center - half, hi = center + half)
}

# Sequences for the ribbons (sorted x is important for ribbons)
n_seq  <- sort(unique(by_class$n))
band95 <- wilson_band(n_seq, p0 = 0.80, level = 0.95)
band80 <- wilson_band(n_seq, p0 = 0.80, level = 0.80)

library(ggplot2)

ggplot(by_class, aes(x = n, y = p)) +
  geom_ribbon(data = band95, aes(x = n, ymin = lo, ymax = hi),
              inherit.aes = FALSE, alpha = 0.10) +
  geom_ribbon(data = band80, aes(x = n, ymin = lo, ymax = hi),
              inherit.aes = FALSE, alpha = 0.20) +
  geom_hline(yintercept = 0.80, linetype = 2, color = "red") +
  geom_point(alpha = 0.7) +
  coord_cartesian(ylim = c(0, 1)) +
  labs(x = "Class sample size (n_j)",
       y = "Empirical coverage of 80% PI",
       title = "Coverage funnel plot with expected 80%/95% bands around nominal 0.80") +
  theme_bw()


```



## Gamma Shape to vary Hierarchically Across Groups

### What "Shape" does

In a Gamma model:

$$
Y_{ij} \sim \operatorname{Gamma}(\text{shape}=k, \text{mean}=\mu_{ij})
$$

with the link function

$$
\log(\mu_{ij}) = \log(\text{Payroll}_{ij}) + X_{ij} \beta + Z_{ij} u_j
$$

the **shape** parameter $k$ controls the **dispersion** (variance) relationship:

$$
\text{Var}(Y_{ij}) = \frac{\mu_{ij}^2}{k} = \phi \mu_{ij}^2
$$

Larger $\phi$ (smaller shape $k$) → larger variance (heavier relative tail).

### Limitations of a _fixed_ shape


```{r}
family = Gamma(link = "log")
# Gamma shape
  prior(lognormal(log(3.3), 0.4), class = shape)
```

the shape is **constant for all classes and years** - a single global dispersion parameter.  This model assumes the _coefficient of variation_ (CV = sd/mean = 1/sqrt(shape)) is constant across all groups.

From the grouped coverage plots, we see some classes are under-dispersed (over-predicted variance) while others are over-dispersed (under-predicted variance):

- classes were over-covered (too wide → model’s variance too large)
- classes were under-covered (too narrow → model’s variance too small)

This suggests that dispersion **differs by class**.

### The conceptual fix: hierarchical (or "random effect") shape

Allow the shape parameter to vary by class, e.g.

$$
\log \left(k_j\right)=\alpha_0+v_j, \quad v_j \sim \mathcal{N}\left(0, \sigma_{\text {shape }}^2\right)
$$

Each class $j$ has its own effective shape parameter $k_j$ controlling its variance, partially pooled toward the global mean shape $\exp(\alpha_0)$.  Analogous to random intercepts in the mean model, but applied to **dispersion**.

- Classes with little data shrink toward the global mean shape, $\alpha_0$ (the global log-shape).
- Classes with rich data learn their own dispersion.

### brms implementation

`brms` allows modeling group-level effects on the shape parameter via the `shape ~ 1 + (1 | Class)` syntax.  These are a separate submodel for the distributional parameters.  The model learns that some classes have tighter (larger shape -> smaller dispersion) or looser (smaller shape -> larger dispersion) Gamma distributions, with partial pooling.

- Each class gets a posterior for its own shape parameter $k_j$.
- Classes with little data shrink toward the global mean shape.
- Class with rich data learn their own dispersion.
- Posterior predictive intervals become wider for classes with high-volatility and narrower for stable classes.

**Actuarial Importance**
- Captures **heterogeneity of risk volatility** across classifications.
- Continue to benefit from **partial pooling**, to preserve the concept of credibility.
- Improves **calibration** and **predictive uncertainty**, particularly for classes with small exposure.
- The mean model is unchanged, only the dispersion structure is modified, keeping the interpretation of fixed and random effects the same.

Two-part formula.

```{r}
#| label: model-3-brms-shape-hierarchical

priors <- c(
  # ---- mean model priors (log-rate link) ----
  prior(student_t(3, log(100), 0.5), class = Intercept),
  prior(normal(0, 0.1), class = b, coef = "Year_C"),

  # random effects for mean
  prior(student_t(3, 0, 1.2), class = sd, group = "Class", coef = "Intercept"),
  prior(student_t(3, 0, 0.6), class = sd, group = "Class", coef = "Year_C"),
  # correlation between random effects
  prior(lkj(2), class = cor, group = "Class"),

  # ---- shape submodel priors (on log(shape)) ----
  # Instead of prior for the fixed shape, we have hierarchical shape submodel
  # The link for "shape" is log (positive parameter),  these priors live on log(shape) scale
  # By default, the random effects in the shape submodel are independent of those in the mean submodel
  # Intercept on log(shape); choose center consistent with prior belief (e.g., log(3.3))
  prior(normal(log(3.3), 0.5), class = Intercept, dpar = "shape"),
  # Random-intercept SD for log(shape) by Class
  prior(student_t(3, 0, 0.5), class = sd, group = "Class", dpar = "shape")
)

mod_3_brms <- brm(
  bf(
    # mean submodel
    Loss ~ offset(log_payroll) + 1 + Year_C + (1 + Year_C | Class),
    # hierarchical dispersion: log(shape) has a random intercept by Class
    shape ~ 1 + (1 | Class)
  ),
  data   = comp_loss,
  family = Gamma(link = "log"),        # mean link; shape uses log link internally
  prior  = priors,
  chains = 4, 
  cores = 4,
  warmup = 1500, 
  iter = 10000,
  seed   = 1821,
  backend = "rstan",
  control = list(adapt_delta = 0.95, max_treedepth = 12),
  save_pars = save_pars(all = TRUE),
  refresh = 0,
  file = "Models/workcomp/mod-3-brms-shape-hierarchical"
)
```


```{r}
re_shape   <- ranef(mod_3_brms, dpar = "shape")$Class[,, "Intercept"]
fix_shape  <- fixef(mod_3_brms, dpar = "shape")["Intercept","Estimate"]
k_class <- exp(fix_shape + re_shape)
phi_class <- 1 / k_class
```

The hierarchical shape improves the coverage spread.  We still need to check the overall global calibration though.

```{r}

# 1) Posterior predictive draws that INCLUDE group effects
#    (re_formula = NULL includes Class REs for the mean AND for the shape submodel)
post <- tidybayes::add_predicted_draws(
  comp_loss,
  mod_3_brms,
  ndraws = 800,
  re_formula = NULL,         # <- important for hierarchical shape
  allow_new_levels = FALSE   # set TRUE only if new Classes appear in comp_loss
)

# 2) Build the 0/1 indicator for "covered by 80% PI" within each observed row
by_obs <- post |>
  group_by(Class, .row) |>
  summarise(
    obs = first(Loss),
    lo  = quantile(.prediction, 0.10),
    hi  = quantile(.prediction, 0.90),
    .groups = "drop"
  ) |>
  mutate(covered = as.integer(obs >= lo & obs <= hi))

# 3) Summarize to class-level and compute Jeffreys intervals for the binomial coverage
by_class <- by_obs |>
  group_by(Class) |>
  summarise(
    n   = n(),
    k   = sum(covered),
    p   = k / n,
    # Jeffreys posterior Beta(k + 0.5, n - k + 0.5)
    lo80 = qbeta(0.10, k + 0.5, n - k + 0.5),
    hi80 = qbeta(0.90, k + 0.5, n - k + 0.5),
    lo95 = qbeta(0.025, k + 0.5, n - k + 0.5),
    hi95 = qbeta(0.975, k + 0.5, n - k + 0.5),
    .groups = "drop"
  )

# 4) Plot: thick bar = 80% CI, thin whisker = 95% CI, point = p_hat
ggplot(by_class, aes(x = p, y = reorder(Class, p))) +
  # 95% CI (thin)
  geom_errorbarh(aes(xmin = lo95, xmax = hi95), height = 0.2, linewidth = 0.25, alpha = 0.6) +
  # 80% CI (thick)
  geom_errorbarh(aes(xmin = lo80, xmax = hi80), height = 0.35, linewidth = 1.1) +
  geom_point(size = 1.4) +
  geom_vline(xintercept = 0.80, linetype = 2, color = "red") +
  labs(
    x = "Empirical coverage of 80% predictive interval",
    y = "Class",
    title = "Class-level calibration (hierarchical Gamma shape): 80% & 95% binomial intervals"
  ) +
  coord_cartesian(xlim = c(0, 1)) +
  theme_bw()
```

**Actuarial Interpretation**

This is a bayesian analogue of allowing credibility-weighted dispersion parameters in a Double-GLM framework.
- Each class's _volatility parameter_ (shape $k_j$) is now partially credible - shrunken toward the overall mean but allowed to deviate if supported by the data.
- This yields **heterockedastic credibility** - groups with limited experience borrow strength; high-volume, high-credibility groups drive their own variance.

```{r}

# --- bands (same formulas) ----------------------------------------------------
wilson_band <- function(n, p0 = 0.80, level = 0.95) {
  z <- qnorm(1 - (1 - level)/2)
  denom  <- 1 + z^2 / n
  center <- (p0 + (z^2/(2*n))) / denom
  half   <- (z * sqrt(p0*(1-p0)/n + z^2/(4*n^2))) / denom
  tibble(n = n, lo = center - half, hi = center + half)
}

# ensure numeric, unique, sorted n
n_seq  <- sort(unique(as.numeric(by_class$n)))
band95 <- wilson_band(n_seq, p0 = 0.80, level = 0.95) |> arrange(n)
band80 <- wilson_band(n_seq, p0 = 0.80, level = 0.80) |> arrange(n)

# --- tag classes as below/inside/above the 95% band ---------------------------
bc2 <- by_class |> 
  dplyr::mutate(n = as.numeric(n)) |> 
  dplyr::left_join(band95, by = "n") |> 
  dplyr::mutate(dir95 = case_when(
    p < lo ~ "below",
    p > hi ~ "above",
    TRUE   ~ "inside"
  )) |> 
  dplyr::select(Class, n, p, lo, hi, dir95)

# --- PRE-COUNT duplicates so point size = how many classes at that (n, p) -----
bc_plot <- bc2 |> 
  count(n, p, dir95, name = "freq") |> 
  arrange(n, p)

# quick tallies for subtitle
tot     <- nrow(bc2)
below   <- sum(bc2$dir95 == "below")
above   <- sum(bc2$dir95 == "above")
outside <- below + above

# --- plot ---------------------------------------------------------------------
ggplot() +
  # ribbons: give x + ymin/ymax; no inherited aes
  geom_ribbon(data = band95, aes(x = n, ymin = lo, ymax = hi),
              inherit.aes = FALSE, alpha = 0.10) +
  geom_ribbon(data = band80, aes(x = n, ymin = lo, ymax = hi),
              inherit.aes = FALSE, alpha = 0.20) +
  geom_hline(yintercept = 0.80, linetype = 2, color = "red") +
  # bubble points: size by 'freq' (pre-counted), color by inside/below/above
  geom_point(data = bc_plot,
             aes(x = n, y = p, size = freq, color = dir95),
             alpha = 0.80) +
  scale_size_area(max_size = 7, breaks = pretty(bc_plot$freq), name = "Classes at point") +
  scale_color_manual(values = c(below = "#1f77b4", inside = "grey40", above = "#d62728"),
                     name = "vs 95% band") +
  coord_cartesian(ylim = c(0, 1)) +
  labs(
    x = "Class sample size (n_j)",
    y = "Empirical coverage of 80% PI",
    title = "Coverage funnel with frequency-sized points",
    subtitle = sprintf("Outside 95%% band: %d/%d (below: %d, above: %d)", outside, tot, below, above)
  ) +
  theme_bw()
```

**"Variance Map"** by Class

Gamma: $\text{Var}(Y \mid \mu, k)=\frac{\mu^2}{k} \rightarrow \text{CV}(Y \mid \mu, k)=\frac{1}{\sqrt{k}}$

So a "variance map" is just the posterior for each class's CV.  Get the posterior draws of the class-specific Gamma shape parameters $k_j$ and convert to $\text{CV}_j = 1/\sqrt{k_j}$.


```{r}
classes_ref <- comp_loss |> 
  dplyr::group_by(Class) |> 
  dplyr::summarise(
    Year_C = 0,
    log_payroll = log(median(Payroll_M, na.rm = TRUE)),
    .groups = "drop"
  )

# Posterior for class-specific shape k_j (on log-scale)
# posterior_linpred(..., transform=TRUE) returns k_j per draw

k_mat <- posterior_linpred(
  mod_3_brms,
  dpar = "shape",
  newdata = classes_ref,
  re_formula = ~ (1 | Class), # include Class RE's for shape
  transform = TRUE,
  ndraws = 2000
)

k_draws <- as.data.frame(k_mat) |> 
  dplyr::mutate(.draw = row_number()) |> 
  tidyr::pivot_longer(cols = - .draw, names_to = "col", values_to = "k") |>
  dplyr::mutate(
    class_index = as.integer(gsub("^V", "", col)),
    Class = classes_ref$Class[class_index],
    CV = 1 / sqrt(k)
  ) |>
  dplyr::select(.draw, Class, k, CV)

cv_sum <- k_draws %>%
  summarise(
    CV_med  = median(CV),
    CV_lo80 = quantile(CV, 0.10),
    CV_hi80 = quantile(CV, 0.90),
    CV_lo95 = quantile(CV, 0.025),
    CV_hi95 = quantile(CV, 0.975),
    .by = Class
  ) %>%
  left_join(comp_loss %>% count(Class, name = "n_j"), by = "Class")

ggplot(cv_sum, aes(x = CV_med, y = reorder(Class, CV_med), color = n_j)) +
  geom_errorbarh(aes(xmin = CV_lo95, xmax = CV_hi95), height = 0.18, linewidth = 0.25, alpha = 0.6) +
  geom_errorbarh(aes(xmin = CV_lo80, xmax = CV_hi80), height = 0.36, linewidth = 1.1) +
  geom_point(size = 1.6) +
  scale_color_viridis_c(option = "C", end = 0.9) +
  labs(x = "Coefficient of variation by Class (CV = 1/√k_j)",
       y = "Class", color = "n_j",
       title = "Variance map: class-specific dispersion (hierarchical Gamma shape)") +
  theme_bw()
```

```{r}
# pick a manageable subset to plot (e.g., 24 largest-n classes)
top_classes <- cv_sum %>% arrange(desc(n_j)) %>% slice_head(n = 24) %>% pull(Class)

# CV
ggplot(filter(k_draws, Class %in% top_classes),
       aes(y = reorder(Class, CV, median), x = CV)) +
  tidybayes::stat_halfeye(.width = c(0.80, 0.95), point_interval = "median_qi") +
  labs(x = "CV = 1/√k_j", y = "Class", title = "Posterior distributions of class-specific CV") +
  theme_bw()

# or shape k
ggplot(filter(k_draws, Class %in% top_classes),
       aes(y = reorder(Class, k, median), x = k)) +
  tidybayes::stat_halfeye(.width = c(0.80, 0.95), point_interval = "median_qi") +
  labs(x = "Gamma shape k_j", y = "Class", title = "Posterior distributions of class-specific shape (k)") +
  theme_bw()

```

**posterior distributions of $k$**


**LOO-PIT**

```{r}
#| label: mod-27-brms-loo-pit-overlay

# compute LOO with momemt matching for large-k points
loo_mm   <- brms::loo(mod_3_brms, moment_match = TRUE, k_threshold = 0.7)

print(loo_mm)
table(loo_mm$diagnostics$pareto_k > 0.7, useNA = "ifany")
```


```{r}
plot(loo_mm)
```

```{r}
#| label: mod-3-brms-loo-pit-overlay-plot
#| fig-cap: LOO-PIT Overlay Plot for Random Shape Effects Model
pp_check(mod_3_brms, type = "loo_pit_overlay") +
  labs(
    title = "LOO-PIT Overlay Plot for Model 3",
  ) +
  theme_minimal()
```

The posterior predictive distribution looks better calibrated overall compared to the fixed-shape model, and matches the empirical distribution of the data across the full range of outcomes.

```{r}
# Use the corrected PSIS weights in the LOO-PIT plot
y     <- comp_loss$Loss
yrep  <- posterior_predict(mod_3_brms, ndraws = 2000)
lw    <- weights(loo_mm$psis_object, normalize = TRUE)   # importance weights for each draw x obs

bayesplot::ppc_loo_pit_overlay(y = y, yrep = yrep, lw = lw)
bayesplot::ppc_loo_pit_qq(y = y, yrep = yrep, lw = lw)

```

```{r}
pp_check(mod_3_brms, moment_match = TRUE, type = "loo_pit_qq") +
  theme_minimal()
```

This confirms the hierarchical shape model improves global calibration compared to the fixed-shape model.  A **fixed Gamma shape** implied a single variance-mean relationship:

$$
\text{Var} (Y \mid \mu) = \frac{\mu^2}{k}
$$

If different classes have different dispersion, a single shape $k$ parameter can't fit them all.  This produces under- or over-dispersion in subsets of the data, leading to mis-calibration and non-uniform LOO-PIT.

Allowing,

$$
\log \left(k_j\right)=\alpha_0+u_j, \quad u_j \sim N\left(0, \sigma_{\text {shape }}^2\right)
$$

lets the model adapt class-specific variance-mean relationships.

```{r}
#| label: mod-3-brms-reloo

loo_rl <- brms::loo(mod_3_brms, reloo = TRUE, k_threshold = 0.7, save_psis = TRUE)
print(loo_rl)
```

Computed from 34000 by 778 log-likelihood matrix.

         Estimate   SE
elpd_loo -10879.1 45.0
p_loo       240.9 16.6
looic     21758.1 90.0
------
MCSE of elpd_loo is 0.8.
MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.8]).

All Pareto k estimates are good (k < 0.7).

```{r}
# y = observed vector; yrep = posterior predictive draws (matrix: ndraws x nobs)
y    <- comp_loss$Loss
yrep <- posterior_predict(mod_3_brms)

lw_rl <- weights(loo_rl$psis_object, normalize = TRUE)

ppc_loo_pit_qq(
  y = y,
  yrep = yrep,
  lw = lw_rl
) + theme_minimal()
```
