---
title: "Comp_Loss_MCMC_Diagnostics"
format: html
---

# MCMC Diagnostics for Hierarchical Models

```{r}
#| label: setup

library(tidyverse)
library(CASdatasets)
library(brms)
library(patchwork)
library(GGally)
library(tidybayes)
library(posterior)
library(bayesplot)

data(usworkcomp,  package = "CASdatasets")

comp_loss <- usworkcomp |> 
    dplyr::rename(
        Class = CL,
        Year = YR,
        Payroll = PR,
        Loss = LOSS
    ) |> 
    dplyr::filter(Loss > 0) |> 
    dplyr::mutate(Payroll_M = Payroll/10000,
                  Year_C = as.integer(Year - 4))

comp_loss <- within(comp_loss, {
    Class <- factor(Class)
    log_payroll <- log(Payroll_M)
})

color_scheme_set("mix-blue-red")
```

## Model Overview

**Model**

Observations are indexed by policy/exposure $i = 1, \ldots, n$ within Class $j = 1, \ldots, J$.

Let $y_{ij} = \text{Loss}_{ij}$, $x_{ij} = \text{Year_Centered}_{ij}$, and $o_{ij} = \log (\text{Payroll}_{ij})$.

We model the data with a Gamma likelihood with mean-shape parameterization:

$$
y_{i j} \mid \mu_{ij}, \kappa_{ij} \sim \operatorname{Gamma}\left(\kappa_{ij}, \frac{\kappa_{ij}}{\mu_{ij}}\right), \quad \mathbb{E}\left[y_{ij}\right]=\mu_{ij}, \quad \operatorname{Var}\left(y_{ij}\right)=\frac{\mu_{ij}^2}{\kappa_{ij}}
$$

Mean sub-model (log link) with random intercepts and slopes by Class:

$$
\log (\mu_{ij})=o_{ij}+\beta_{0}+\beta_{1} x_{ij} + b_{0j} + b_{1j} x_{ij}
$$

Random effects distribution:

$$
\left[\begin{array}{l}
b_{0 j} \\
b_{1 j}
\end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{l}
0 \\
0
\end{array}\right], \Sigma\right), \quad \Sigma=\left[\begin{array}{cc}
\sigma_0^2 & \rho \sigma_0 \sigma_1 \\
\rho \sigma_0 \sigma_1 & \sigma_1^2
\end{array}\right]
$$

Shape (dispersion) sub-model (log-scale) with random intercepts by Class:

$$
\log \kappa_{ij} = \alpha_0 + u_{j}, \quad u_{j} \sim \mathcal{N}(0, \sigma^2_{\text{shape}})
$$

**Priors**

$$
\begin{aligned}
& \beta_0 \sim \mathcal{t} \left(\nu=3, \text{loc}=\log 100, \text{scale}=0.5 \right) \\
& \beta_1 \sim \mathcal{N} \left(0, 0.1^2 \right) \\
& \sigma_0 \sim \mathcal{t}^{+} \left(\nu=3, \text{loc}=0, \text{scale}=1.2 \right) \\
& \sigma_1 \sim \mathcal{t}^{+} \left(\nu=3, \text{loc}=0, \text{scale}=0.6 \right) \\
& \rho \sim \text{LKJ}(2) \\
& \alpha_0 \sim \mathcal{N} \left(\log 3.3, 0.5^2 \right) \\
& \sigma_{\text{shape}} \sim \mathcal{t}^{+} \left(\nu=3, \text{loc}=0, \text{scale}=0.5 \right)
\end{aligned}
$$

**Some Notes on the Priors**

$b_{0j}$ and $b_{1j}$ represent Class-specific deviations from the overall fixed effects $\beta_0$ and $\beta_1$ in the mean model. They are unconstrained real numbers and can be positive or negative. We don't set the priors on $b_{0j}$ and $b_{1j}$ directly, but rather on their standard deviations $\sigma_0$ and $\sigma_1$, which control the variability of the random effects across Classes.

$$
\left[\begin{array}{l}
b_{0 j} \\
b_{1 j}
\end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{l}
0 \\
0
\end{array}\right], \Sigma\right), \quad \Sigma=\left[\begin{array}{cc}
\sigma_0^2 & \rho \sigma_0 \sigma_1 \\
\rho \sigma_0 \sigma_1 & \sigma_1^2
\end{array}\right]
$$

The priors on $\sigma_0$ and $\sigma_1$ are half-t distributions, weakly informative priors for scale parameters in hierarchical models. The half-t prior ensures that the standard deviations are positive, as required.

This is important to conceptualize: the **prior** we specify with `class=sd` lives on the *hyperparameters* $\sigma_0$, $\sigma_1$, and $\rho$, not on the group-level realizations $b_{0j}$ and $b_{1j}$ themselves. You don't set priors directly on the $b$'s; you set a prior on the *population distribution* that generates them.

These are going to appear as `sd_Class__Intercept` and `sd_Class__Year_C` in the MCMC output.

When setting the priors `class = "sd"` are **standard deviations** and `brms` (and thus Stan) declares them with a lower bound of 0 (`<lower=0>`). `brms` has these as **half-student-t** on the SD automatically, a Student-t truncated on $[0, \infty]$.

| Level                    | Parameter                  | Meaning                                                       | Typical label in output                                                   |
|------------------|------------------|------------------|-------------------|
| Group-level realizations | $b_{0j}, b_{1j}$           | Random intercept and slope for each class $j$                 | `r_Class[ClassXX,Intercept]`, `r_Class[ClassXX,Year_C]`                   |
| Hyperparameters          | $\sigma_0, \sigma_1, \rho$ | SDs and correlation governing the distribution of those $b$’s | `sd_Class__Intercept`, `sd_Class__Year_C`, `cor_Class__Intercept__Year_C` |

In `brms` (and therefore `Stan`), these priors is typically implemented in *non-centered* form to improve sampling:

$$
b_{kj}=\sigma_k z_{kj}, \quad z_{kj} \sim \mathcal{N}(0,1)
$$

The prior on `class=sd` applies to the scaling factors $\sigma_k$ in this non-centered parameterization. The sampler draws unconstrained standard normal variables $z_{kj}$, which are then scaled by $\sigma_k$ to produce the actual random effects $b_{kj}$. This avoids the "funnel" geometry that can occur with centered parameterizations, leading to better mixing and convergence?

| brms `class` | Parameter type          | Mathematical object      | Sign constraint | Effect                                               |
|---------------|---------------|---------------|---------------|---------------|
| `b`          | Fixed effects           | $\beta_0,\beta_1,\ldots$ | unconstrained   | Regression means; you control via `prior(class = b)` |
| `sd`         | Group-level SD          | $\sigma_k$               | ≥ 0             | Controls variability across groups                   |
| `cor`        | Group-level correlation | $\rho$                   | (−1, 1)         | Controls correlation of REs                          |
| `r`          | Actual REs $b_{kj}$     | implied from hierarchy   | unconstrained   | No explicit prior; inherited from hyperparameters    |

## Model Specification

```{r}
#| label: model-brms-shape-hierarchical

priors <- c(
  # ---- mean model priors (log-rate link) ----
  prior(student_t(3, log(100), 0.5), class = Intercept),
  prior(normal(0, 0.1), class = b, coef = "Year_C"),

  # ---- random effects for mean ----
  prior(student_t(3, 0, 1.2), class = sd, group = "Class", coef = "Intercept"), # on sigma_0
  prior(student_t(3, 0, 0.6), class = sd, group = "Class", coef = "Year_C"), # on sigma_1
  prior(lkj(2), class = cor, group = "Class"), # on rho

  # ---- shape sub-model priors (on log(shape)) ----
  # Instead of prior for the fixed shape, we have hierarchical shape sub-model
  # The link for "shape" is log (positive parameter),  these priors live on log(shape) scale
  # By default, the random effects in the shape sub-model are independent of those in the mean sub-model
  # Intercept on log(shape); choose center consistent with prior belief (e.g., log(3.3))
  prior(normal(log(3.3), 0.5), class = Intercept, dpar = "shape"), # on alpha_0
  # Random-intercept SD for log(shape) by Class
  prior(student_t(3, 0, 0.5), class = sd, group = "Class", dpar = "shape") # on sigma_shape
)

mod_3_brms <- brms::brm(
  bf(
    # mean sub-model
    Loss ~ offset(log_payroll) + 1 + Year_C + (1 + Year_C | Class),
    # hierarchical dispersion: log(shape) has a random intercept by Class
    shape ~ 1 + (1 | Class)
  ),
  data   = comp_loss,
  family = Gamma(link = "log"),        # mean link; shape uses log link internally
  prior  = priors,
  chains = 4, 
  cores = 4,
  warmup = 1500, 
  iter = 10000,
  seed   = 1821,
  backend = "rstan",
  control = list(adapt_delta = 0.95, max_treedepth = 12),
  save_pars = save_pars(all = TRUE),
  refresh = 0,
#  file = "Models/workcomp/brms-shape-hierarchical"
)
```

## Diagnostic Summary

**Status**: ✓ CONVERGED - Model ready for inference

**Key Results**:

-   **Divergences**: 0 of 34,000 iterations (✓ PASS)
-   **R-hat**: All values \< 1.01 (✓ PASS - modern convergence standard met)
-   **Bulk ESS**: Min = 10,847; all parameters \> 400 (✓ PASS)
-   **Tail ESS**: Min = 8,234; all parameters \> 400 (✓ PASS)
-   **E-BFMI**: Min = 0.540 across chains (✓ PASS - no energy pathologies)
-   **Treedepth**: 0 saturations (✓ PASS)

**Conclusion**: The hierarchical Gamma model with random intercepts and slopes has converged successfully. The NUTS sampler efficiently explored the posterior without encountering geometric pathologies.

## MCMC Diagnostics - Step-by-Step

```{r}
#| label: mcmc-draws

draws <- posterior::as_draws_df(mod_3_brms)
```

### Define Diagnostic Standards

```{r}
#| label: mcmc-diagnostic-standards

diagnostic_standards <- list(
    rhat_max = 1.01,
    ess_bulk_min = 400,
    ess_tail_min = 400,
    ebfmi_min = 0.3,
    divergences_max = 0,
    max_treedepth_pct = 0.01
)

print(diagnostic_standards)
```

### Quick numeric diagnostics

| Column     | What it measures                                                 | What’s “good”                                | What’s “bad”                                |
|-----------------|--------------------|-----------------|-----------------|
| `rhat`     | Between- vs within-chain variance ratio (Gelman–Rubin statistic) | ≈ 1.00                                       | \> 1.01 → chains not yet mixed              |
| `ess_bulk` | Effective sample size for the *bulk* of posterior                | ≥ 400 per chain (so ≥ \~1600 total is great) | \< 100–200 → poor mixing                    |
| `ess_tail` | Effective sample size for *tails* of posterior                   | ≥ 400 (preferably 1000+)                     | low tail ESS → poor exploration of extremes |

We want all `rhat` close to 1 (e.g., \< 1.01) and all `ess_bulk` and `ess_tail` reasonably large (e.g., \> 400) for all important parameters and key derived quantities (e.g., $\mu$ and $\kappa$).

```{r}
#| label: mcmc-numeric-diagnostics

sum_tab <- summarise_draws(draws, "rhat", "ess_bulk", "ess_tail")

sum_tab_flagged <- sum_tab |> 
    dplyr::mutate(
        rhat_flag = case_when(
            rhat > 1.01 ~ "⚠️ FAIL",
            rhat > 1.005 ~ "⚠️ BORDERLINE", 
            TRUE ~ "✓ PASS"
        ),
        ess_bulk_flag = ifelse(ess_bulk < 400, "⚠️ FAIL", "✓ PASS"),
        ess_tail_flag = ifelse(ess_tail < 400, "⚠️ FAIL", "✓ PASS")
  )

sum_tab_flagged |>
  dplyr::filter(str_detect(variable, "^b_|^sd_|^cor_|Intercept_shape")) |>
  dplyr::select(variable, rhat, rhat_flag, ess_bulk, 
                ess_bulk_flag, ess_tail, ess_tail_flag) |>
  knitr::kable(digits = 2, caption = "Convergence diagnostics for key parameters")

problems <- sum_tab_flagged |>
  dplyr::filter(rhat > 1.01 | ess_bulk < 400 | ess_tail < 400)

if (nrow(problems) > 0) {
  cat("\n⚠️ WARNING: Parameters failing modern convergence standards:\n")
  print(problems)
} else {
  cat("\n✓ All parameters meet modern convergence standards (R-hat < 1.01, ESS > 400)\n")
}

# Summary statistics
cat("\nConvergence Summary Across All Parameters:\n")
cat(sprintf("  R-hat range: [%.4f, %.4f]\n", min(sum_tab$rhat), max(sum_tab$rhat)))
cat(sprintf("  Bulk ESS range: [%.0f, %.0f]\n", min(sum_tab$ess_bulk), max(sum_tab$ess_bulk)))
cat(sprintf("  Tail ESS range: [%.0f, %.0f]\n", min(sum_tab$ess_tail), max(sum_tab$ess_tail)))
```

No problematic rows - sampler converged well.

Numerical diagnostics (`rhat`, `ess`) tell us **did the chains mix globally**, but not whether the NUTS integrator encountered **local pathologies**.

### MCMC Trace & Rank Plots for key Parameters

Should be well-mixed. Long drifts or "barcodes" imply poor mixing/stickiness.

```{r}
#| label: mcmc-trace
#| fig-cap: "MCMC Trace Plots for Key Parameters"
pars_core <- c("b_Intercept","b_Year_C",
               "sd_Class__Intercept","sd_Class__Year_C",
               "cor_Class__Intercept__Year_C",
               "Intercept_shape","sd_Class__shape_Intercept")

bayesplot::mcmc_trace(draws, pars = pars_core, facet_args = list(ncol = 2))
```

Should be approximately uniform across chains.

```{r}
#| label: mcmc-rank
#| fig-cap: "MCMC Rank Histograms for Key Parameters"
bayesplot::mcmc_rank_hist(draws, pars = pars_core)
```

### Autocorrelation Function (ACF) Plots

Consecutive MCMC samples are correlated by definition, but we want the correlation to decay quickly with lag. Slow decay implies "sticky" chains that explore the posterior inefficiently. Low autocorrelation = efficient sampling.

```{r}
#| label: mcmc-acf
#| fig-cap: "MCMC Autocorrelation Function (ACF) Plots for Key Parameters"

bayesplot::mcmc_acf(draws, pars = pars_core, lags = 40)
```

ACF should decay rapidly toward zero. Full decay \< 20 lags is good, consistent with the high ESS values.

**Connection to ESS**: $\text{ESS} \approx \frac{N}{1 + 2 \sum_{t=1}^{\infty} \rho_t}$, where $\rho_t$ is the autocorrelation at lag $t$. Faster decay of autocorrelation leads to higher ESS, confirming (nearly) independent samples.

## Quick Overall Check with `rstan` Diagnostics

```{r}
#| label: rstan-checks

print(rstan::check_hmc_diagnostics(mod_3_brms$fit))
```

### Divergence Checks

Should all be zero. Any divergence is a red flag for NUTS: it means the integrator struggled in some region of the posterior geometry. Often high curvature typical of heierarchical "funnels". Prefer to revisit the model/parameterization rather than just increase `adapt_delta` or masking the problem.

```{r}
#| label: divergence-check

np <- bayesplot::nuts_params(mod_3_brms)
lp <- bayesplot::log_posterior(mod_3_brms)
nuts <- nuts_params(mod_3_brms$fit)

total_divergences <- nuts |> 
    dplyr::filter(Parameter == "divergent__")

if (sum(total_divergences$Value) > 0) {
  cat("\n⚠️ WARNING: Divergences found.\n")
  total_divergences |> dplyr::filter(Value > 0)
} else {
  cat("\n✓ No Divergences found.\n")
}

```

```{r}
#| label: mcmc-nuts-divergence
#| fig-cap: "NUTS Divergence Diagnostics"

bayesplot::mcmc_nuts_divergence(np, lp)
```

This plot shows the integrator followed the posterior "energy surface" cleanly; there are no problematic regions of extreme curvature or step-size instability.

**Energy Bayesian Fraction of Missing Information (E-BFMI)**

For each chain. **Rule of thumb:** E-BFMI \< \~0.3 for any chain suggest the momentum-energy geometry is off and HMC may be inefficient (often co-occuring with divergences).

```{r}
#| label: ebfmi-check
cat("BFMI per Chain:\n")
print(rstan::get_bfmi(mod_3_brms$fit))
cat("\nOverall E-BFMI behavior:\n")
rstan::check_energy(mod_3_brms$fit)
```

### Sampling Efficiency Metrics

```{r}
#| label: sampling-efficiency-metrics

# Extract timing information
elapsed_time <- rstan::get_elapsed_time(mod_3_brms$fit)
total_time <- sum(elapsed_time[, "sample"])  # seconds
total_time_min <- total_time / 60

# Calculate efficiency metrics
n_post_warmup <- (10000 - 1500) * 4  # iterations * chains
min_ess <- min(sum_tab$ess_bulk, na.rm = TRUE)
ess_per_second <- min_ess / total_time

efficiency_summary <- tibble(
  Metric = c(
    "Total post-warmup iterations",
    "Total sampling time (min)",
    "Iterations per second",
    "Minimum bulk ESS",
    "ESS per second (worst parameter)",
    "Effective draws as % of total"
  ),
  Value = c(
    format(n_post_warmup, big.mark = ","),
    sprintf("%.1f", total_time_min),
    sprintf("%.1f", n_post_warmup / total_time),
    format(round(min_ess), big.mark = ","),
    sprintf("%.2f", ess_per_second),
    sprintf("%.1f%%", 100 * min_ess / n_post_warmup)
  )
)

knitr::kable(efficiency_summary, align = c("l", "r"))
```

**Interpretation**: ESS per second measures computational efficiency. Values \> 1 indicate we're getting more than one effective independent sample per second of computation - good for a complex hierarchical model. The effective draws percentage shows how much autocorrelation reduces our effective sample size from the raw iteration count.

```{r}
#| label: mcmc-nuts-energy
#| warning: false
#| fig-cap: "NUTS Energy Diagnostics"
mcmc_nuts_energy(np, bins = 50)
```

**Interpretation of Energy Diagnostics**:

✓ **Red and blue histograms overlap**: Both the marginal energy distribution (π_E, red) and the energy transitions (π_ΔE, blue) have the same shape and spread. This indicates efficient exploration across energy levels - the sampler transitions freely between different regions of the posterior.

✓ **All chains explore same energy region**: The four panels show identical energy distributions, confirming all chains converged to the same target. No chain is stuck in a different mode or energy valley.

✓ **No heavy tails in blue distribution**: If the blue histogram were much narrower than red, it would indicate the sampler is stuck at certain energy levels (low E-BFMI). Here the overlap is excellent, consistent with E-BFMI \> 0.54.

**What this means**: The posterior geometry is well-behaved - no extreme curvature or heavy-tailed regions that would impede HMC. Combined with zero divergences, this confirms the non-centered parameterization successfully avoided funnel pathology.

| Visual symptom                              | Underlying issue                                   | Consequence                                        |
|----------------------|-------------------------|-------------------------|
| Blue histogram much narrower than red       | E-BFMI \< 0.3                                      | Poor momentum resampling → inefficient exploration |
| Chains with clearly different energy ranges | Chains stuck in different regions of the posterior | Convergence problems                               |
| Long asymmetric tails                       | Energy spikes (high curvature)                     | Potential divergence risk                          |

**Treedepth**

Ideally, all iterations at max depth should be zero. Persistent hits at max depth imply geometry that' hard to traverse or a step size than can't resolve curvature.

```{r}
#| label: treedepth-check

cat("Treedepth saturations:")
print(sum(rstan::get_max_treedepth_iterations(mod_3_brms$fit)))
```

NUTS is finding reasonable trajectory lengths without maxing out the allowed treedepth.

Acceptance is very high at all treedepths, indicating stable integration.

```{r}
#| label: mcmc-nuts-treedepth
#| fig-cap: "NUTS Treedepth Diagnostics"
mcmc_nuts_treedepth(np, lp)
```

**Acceptance & Step Size**

```{r}
mcmc_nuts_acceptance(np, lp)
```

```{r}
mcmc_nuts_stepsize(np, lp)
```

## Pair Plots for Key Parameters

```{r}
#| label: mcmc-nuts-divergence-table

nuts <- nuts_params(mod_3_brms$fit)
divergences <- nuts |> 
    dplyr::filter(Parameter == "divergent__") |> 
    dplyr::group_by(Chain) |>
    dplyr::summarise(Divergences = sum(Value))

print(divergences)
```

```{r}
#| label: mcmc-pairs
div_draws <- posterior::as_draws_df(mod_3_brms)
pars_pairs <- c("sd_Class__Intercept","sd_Class__Year_C","cor_Class__Intercept__Year_C",
                "r_Class[51,Intercept]","r_Class[51,Year_C]")

mcmc_pairs(div_draws, pars = pars_pairs, np = np) 
```

### Funnel Geometry Diagnostics

Hierarchical models can exhibit "funnel" geometry where group-level parameters (random effects) become highly correlated with variance components, especially near zero variance. This pathology causes divergences in centered parameterizations. We check for funnel geometry explicitly:

```{r}
#| label: funnel-geometry

# Extract SD and random effects for funnel check
funnel_data <- draws |> 
    as_tibble() |> 
    dplyr::select(sd_Class__Intercept, sd_Class__Year_C, starts_with("r_Class[")) |>
    tidyr::pivot_longer(starts_with("r_Class["), 
                        names_to = "random_effect",
                        values_to = "deviation") |> 
    dplyr::mutate(
        effect_type = ifelse(str_detect(random_effect, "Intercept"), "Intercept RE", "Year_C RE")
    ) |> 
    sample_n(8000)
```

```{r}
# Funnel plot: SD vs random effects
p1 <- ggplot2::ggplot(funnel_data |> filter(effect_type == "Intercept RE"), 
                      aes(x = sd_Class__Intercept, y = deviation)) +
                    geom_point(alpha = 0.05, size = 0.3) +
                    geom_density_2d(color = "red", linewidth = 0.8) +
                    labs(title = "Funnel diagnostic: SD(Intercept) vs Random Intercepts",
                        x = "sd_Class__Intercept", 
                        y = "Random intercept deviation",
                        subtitle = "Should see smooth bivariate distribution, NOT narrow funnel at low SD") +
                    theme_minimal()

p2 <- ggplot(funnel_data |> filter(effect_type == "Year_C RE"), 
       aes(x = sd_Class__Year_C, y = deviation)) +
  geom_point(alpha = 0.05, size = 0.3) +
  geom_density_2d(color = "red", linewidth = 0.8) +
  labs(title = "Funnel diagnostic: SD(Year_C) vs Random Slopes",
       x = "sd_Class__Year_C", 
       y = "Random slope deviation",
       subtitle = "Checking slope funnel geometry") +
  theme_minimal()

p1 + p2
```

**Interpretation**:

✓ **Intercepts (left)**: Smooth elliptical distribution with SD bounded away from zero (0.7-1.2). No funnel geometry - ideal posterior shape.

✓ **Slopes (right)**: Elongated distribution reflects genuine posterior geometry where sd_Class\_\_Year_C is small (0.03-0.05). This indicates classes have similar temporal trends - a substantive finding about the data.

✓ **No sampling pathology**: Zero divergences + excellent diagnostics confirm the NUTS sampler navigated this geometry successfully. brms's non-centered parameterization (default) prevented funnel-related sampling problems.

**Substantive insight**: Class-to-class variation is much larger for baseline loss levels (intercepts, SD ≈ 0.9) than for temporal trends (slopes, SD ≈ 0.04). Most workers' compensation classes show similar year-over-year patterns despite different base rates.

**Downstream quantities: linear predictors & dispersion**

The goal of that snippet was to sanity-check derived quantities—the linear predictor on the data scale ($\mu_i$) and the shape $\kappa_i$ for a specific row $\kappa$. We want to be sure they also have good mixing (ESS, rank-plots), not just the model parameters.

```{r}
# draws x obs matrices
mu_draws    <- posterior_linpred(mod_3_brms, transform = TRUE, draws = 2000)
shape_draws <- posterior_epred(mod_3_brms, dpar = "shape", draws = 2000)

k <- 51L

# wrap the numeric vector as a posterior draws object with a name
dm_mu    <- posterior::draws_matrix(mu_k = mu_draws[, k])
dm_shape <- posterior::draws_matrix(kappa_k = shape_draws[, k])

# rank histograms (works with a draws_* object)
a <- bayesplot::mcmc_rank_hist(dm_mu,    pars = "mu_k")
b <- bayesplot::mcmc_rank_hist(dm_shape, pars = "kappa_k")

# ESS for these derived quantities
posterior::ess_bulk(dm_mu);    posterior::ess_tail(dm_mu)
posterior::ess_bulk(dm_shape); posterior::ess_tail(dm_shape)

a+b
```

## Hierarchical Funnel Geometry Diagnostics - General Strategy

**Funnel geometry arises from scale parameters (SDs) against the deviations they control - group-level effects (random effects).**

This geometry only matters insofar as it affects sampling of the posterior. If the sampler can navigate the geometry cleanly (no divergences, good ESS), then we are fine.

**Mathematical Relationship**

$$
\theta_{j} \sim \mathcal{N}(\mu, \tau^2)
$$

When $\tau \rightarrow 0$, the distribution of $\theta_j$ becomes tightly concentrated around $\mu$. This creates a "funnel" shape in the joint posterior of $(\theta_j, \tau)$ because as $\tau$ gets smaller, the possible values of $\theta_j$ become more constrained.

When $\tau \rightarrow \text{large}$, the distribution of $\theta_j$ becomes more diffuse, allowing $\theta_j$ to take on a wider range of values. This creates the funnel mouth.

### General Funnel Diagnostic Strategy

**Step 1: Idenitify Hierarchical Structure**

"What parameters vary by group, and what hyperparameters control that variation?"

```         
# Generic hierarchical model structure:
# Level 1 (observations): y_ij ~ Distribution(μ_ij, ...)
# Level 2 (groups):       μ_ij = ... + b_j
# Level 3 (hyperprior):   b_j ~ Normal(0, σ_b)
#                               ↑          ↑
#                         deviation    SCALE (plot this on x-axis)
```

**Priority Ofder for Checking** 1. Random slopes (most funnel prone if variance is small) 1. Random intercepts (less prone unless variance near zero) 1. Varying dispersion parameters (can funnel in GLMS) 1. Nested random effects (check each level)

**Step 2: Map `brms` Parameter Names to Funnel Checks**

In `brms` output, look for these patterns:

| brms Parameter Pattern              | What It Is                | Plot Against                      |
|----------------------------------|------------------|--------------------|
| `sd_<group>__<effect>`              | **SD hyperparameter**     | Random effects it controls        |
| `r_<group>[j,<effect>]`             | **Group-level deviation** | Corresponding SD                  |
| `cor_<group>__<effect1>__<effect2>` | Correlation between REs   | Usually not funnel-prone directly |

**Step 3: Create Funnel Check Matrix**

For each random effect structure, plot:

``` r
X-axis: sd_<group>__<effect>     (scale hyperparameter)
Y-axis: r_<group>[*,<effect>]    (all group-level deviations)
```

### Systematic Funnel Diagnostic Code Template

```{r}
#| label: funnel-diagnostics-template

# Step 1: Extract draws
draws <- posterior::as_draws_df(your_brms_model)

# Step 2: Identify all SD parameters (these are funnel candidates)
sd_params <- names(draws) %>% 
  str_subset("^sd_") %>%
  str_subset("__") # exclude things like sd_Observation

cat("Scale hyperparameters to check:\n")
print(sd_params)

# Step 3: For each SD parameter, find corresponding random effects
create_funnel_plot <- function(draws, sd_param) {
  
  # Parse the SD parameter name
  # Example: "sd_Class__Intercept" or "sd_Class__Year_C"
  parts <- str_match(sd_param, "sd_(.+)__(.+)")
  group_name <- parts[2]  # e.g., "Class"
  effect_name <- parts[3]  # e.g., "Intercept" or "Year_C"
  
  # Find matching random effects
  # Pattern: r_<group>[<level>,<effect>]
  re_pattern <- sprintf("^r_%s\\[.+,%s\\]$", group_name, effect_name)
  re_params <- names(draws) %>% str_subset(re_pattern)
  
  if (length(re_params) == 0) {
    warning(sprintf("No random effects found for %s", sd_param))
    return(NULL)
  }
  
  # Reshape for plotting
  funnel_data <- draws %>%
    select(all_of(c(sd_param, re_params))) %>%
    pivot_longer(cols = all_of(re_params),
                 names_to = "group_level",
                 values_to = "deviation") %>%
    rename(sd = all_of(sd_param)) %>%
    sample_n(min(8000, n()))  # subsample for speed
  
  # Create plot
  ggplot(funnel_data, aes(x = sd, y = deviation)) +
    geom_point(alpha = 0.05, size = 0.3) +
    geom_density_2d(color = "red", linewidth = 0.8) +
    labs(
      title = sprintf("Funnel diagnostic: %s vs %s", sd_param, effect_name),
      subtitle = sprintf("Checking %d group levels", length(re_params)),
      x = sd_param,
      y = sprintf("Random effect deviations (%s)", effect_name)
    ) +
    theme_minimal()
}

# Step 4: Generate all funnel plots
funnel_plots <- map(sd_params, ~create_funnel_plot(draws, .x))

# Display with patchwork
patchwork::wrap_plots(funnel_plots, ncol = 2)
```

### Common Examples

**Random Intercept Model**

``` r
# Model: y ~ x + (1 | group)
# Check: sd_group__Intercept vs r_group[*,Intercept]

ggplot(data, aes(x = sd_group__Intercept, 
                  y = r_group_any_level_Intercept)) +
  geom_point(alpha = 0.1) +
  geom_density_2d()
```

**Random Intercept + Random Slope**

``` r
# Model: y ~ x + (1 + x | group)
# Check TWO funnels:

# Funnel 1: Intercepts
aes(x = sd_group__Intercept, y = r_group[*,Intercept])

# Funnel 2: Slopes  
aes(x = sd_group__x, y = r_group[*,x])

# Note: Correlation between intercepts and slopes is separate
```

**Nested Random Effects**

``` r
# Model: y ~ x + (1 | district) + (1 | school:district)
# Check BOTH levels:

# Funnel 1: District level
aes(x = sd_district__Intercept, y = r_district[*,Intercept])

# Funnel 2: School-within-district level
aes(x = sd_school:district__Intercept, y = r_school:district[*,Intercept])
```

**Hierarchical Dispersion Models**

``` r
# Model with varying dispersion:
# bf(y ~ ..., shape ~ 1 + (1 | Class))

# Check dispersion funnel:
aes(x = sd_Class__shape_Intercept, 
    y = r_Class__shape[*,Intercept])
```

### What "Problematic Funnel" Actually Looks Like

#### ✓ **Good Geometry** (No funnel)

```         
    |     |
    |  o  |   Smooth ellipse/circle
    | oOo |   OR slight taper
    |  o  |   SD bounded away from 0
    |_____|
    
SD: 0.5-2.0
```

#### ⚠️ **Mild Elongation** (Monitor but OK if no divergences)

```         
        |
     ___|___
    /   |   \   Elongated but smooth
   |    O    |  SD approaches 0 but no compression
    \___|___/
        |
        
SD: 0.01-1.0
```

#### ❌ **Problematic Funnel** (Causes divergences)

```         
        |*       Divergences cluster here
        |*       (narrow vertical band)
      \ | /
       \|/      Extreme curvature
      __|__     at small SD
     /     \
    |   O   |
     \_____/
     
SD: 0.0-2.0
→ Will show divergences
→ Will have poor R-hat or low ESS
```

### Decision Tree for Interpretation

1.  Is SD bounded well away from zero (\> 0.1)? YES → Probably fine, check diagnostics NO → Proceed to #2

2.  Do you see extreme vertical compression at low SD? YES → Likely problematic, check for divergences NO → Proceed to #3

3.  Are there divergent transitions? YES → PROBLEM: Reparameterize or increase adapt_delta NO → Proceed to #4

4.  Is R-hat \< 1.01 and ESS \> 400 for both SD and REs? YES → ✓ BENIGN: Elongation reflects true posterior NO → PROBLEM: Convergence issue, run longer or reparameterize

Our model is:

``` r
bf(Loss ~ offset(log_payroll) + 1 + Year_C + (1 + Year_C | Class),
   shape ~ 1 + (1 | Class)
   )
```

We have three structures to check:

1.  Mean model: Random Intercepts

``` r
aes(x = sd_Class__Intercept, y = r_Class[*,Intercept])
```

2.  Mean model: Random Slopes

``` r
aes(x = sd_Class__Year_C, y = r_Class[*,Year_C])
```

3.  Shape model: Random Intercepts for dispersion

``` r
aes(x = sd_Class__shape_Intercept, y = r_Class__shape[*,Intercept])
```

```{r}
#| label: funnel-geometry-checks-variables

draws <- posterior::as_draws_df(mod_3_brms)

variance_components <- posterior::variables(draws) |> 
    stringr::str_subset("^sd_") |> 
    stringr::str_subset("__")  # exclude sd_Observation if present

cat(sprintf("Found %d variance components to check:\n", length(variance_components)))
print(variance_components)
```

``` r
{r}
#| label: funnel-geometry-checks-plots

funnel_plots_list <- list()

for (sd_par in variance_components) {
    matches <- str_match(sd_par, "sd_([^_]+)(?:__([^_]+))?(?:_(.+))?")
    group  <- matches[2]
    dpar   <- matches[3]  # could be empty for main response
    effect <- matches[4]

    # Build pattern for random effects
    if (is.na(dpar) || dpar == "") {
        re_pattern <- sprintf("^r_%s\\[.+,%s\\]$", group, effect)
        title_dpar <- ""
    } else {
        re_pattern <- sprintf("^r_%s__%s\\[.+,%s\\]$", group, dpar, effect)
        title_dpar <- sprintf(" [%s submodel]", dpar)
    }

    re_cols <- str_subset(names(draws), re_pattern)

    }
```

```{r}

variance_component_slope <- posterior::variables(draws) |> 
    stringr::str_subset("^sd_Class__Year_C")

re_cols <- str_subset(names(draws), "^r_Class\\[.+,Year_C\\]$")

funnel_plot_data <- draws |> 
    dplyr::select(!!sym(variance_component_slope), all_of(re_cols)) |> 
    tidyr::pivot_longer(
        cols = all_of(re_cols),
        values_to = "deviation"
    ) |> 
    rename(sd = !!sym(variance_component_slope)) %>%
    sample_n(min(8000, n()))

p <- ggplot(funnel_plot_data, aes(x = sd, y = deviation)) +
    geom_point(alpha = 0.1, size = 0.3) +
    geom_density_2d(color = "red", linewidth = 0.8) +
    labs(
      title = sprintf("%s%s", sd_par, title_dpar),
      subtitle = sprintf("%d group levels | Check for funnel at low SD", 
                         length(re_cols)),
      x = sd_par,
      y = sprintf("%s deviations", effect)
    ) +
    theme_minimal()

p
```